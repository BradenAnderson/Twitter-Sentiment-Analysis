{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_2_Data_Cleaning_Spacy_and_Spellcheck.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPaBptEX9u2rC/YoYKQUReo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BradenAnderson/Twitter-Sentiment-Analysis/blob/main/01_2_Data_Cleaning_Spacy_and_Spellcheck.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptN6xZ6Kcjnc"
      },
      "source": [
        "# Caution\n",
        "\n",
        "Due to the method used to process the contextualspellCheck spelling suggestions, this notebook may create a large number of spacy doc objects and in doing so utilize a large amount of RAM. Therefore, you should not use this notebook to perform datacleaning on the entire notebook at once. The test drive function at the end of the notebook can be used to perform data cleaning on a specified number of rows (50 rows at a time should work no problem), which will help in understanding the behavior. \n",
        "\n",
        "Additionally, the contextualSpellCheck library seems to be providing unexpected/incorrect results for the spelling predictions. \n",
        "\n",
        "The test drive function will create an output csv file containing a row for each spelling change made during the test. Each row in the file that contains three items: 1) the entire original tweet text, 2) the word that was flagged as being spelled incorrectly, 3) the top spelling suggestion to correct the mistake. \n",
        "\n",
        "The output csv file is titled \"num1_to_num2_spellcheck_test_results.csv\" where num1 and num2 are the starting and ending row numbers in the original tweet dataframe that were used in the test. \n",
        "\n",
        "To understand the nature of these unexpected spelling suggestions, run the test drive function view this output csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOvcOUW_o5oh",
        "outputId": "83c9c295-8fa7-4480-d40c-2575c7509263"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0dU4ZDtFAp_",
        "outputId": "306341d4-fbcb-41f1-9967-629c8ff8a2be"
      },
      "source": [
        "!pip install contextualSpellCheck"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: contextualSpellCheck in /usr/local/lib/python3.7/dist-packages (0.4.1)\n",
            "Requirement already satisfied: transformers>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from contextualSpellCheck) (4.4.2)\n",
            "Requirement already satisfied: spacy>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from contextualSpellCheck) (3.0.5)\n",
            "Requirement already satisfied: editdistance==0.5.3 in /usr/local/lib/python3.7/dist-packages (from contextualSpellCheck) (0.5.3)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from contextualSpellCheck) (1.8.0+cu101)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->contextualSpellCheck) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->contextualSpellCheck) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->contextualSpellCheck) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->contextualSpellCheck) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->contextualSpellCheck) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->contextualSpellCheck) (3.7.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->contextualSpellCheck) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->contextualSpellCheck) (0.0.43)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->contextualSpellCheck) (0.10.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (0.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (1.0.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (0.4.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (2.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (2.0.1)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (1.7.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (54.1.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (0.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (3.7.4.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (2.0.5)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (0.3.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (8.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->contextualSpellCheck) (3.0.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers>=4.0.0->contextualSpellCheck) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.0.0->contextualSpellCheck) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.0.0->contextualSpellCheck) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.0.0->contextualSpellCheck) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.0.0->contextualSpellCheck) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers>=4.0.0->contextualSpellCheck) (3.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->contextualSpellCheck) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->contextualSpellCheck) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->contextualSpellCheck) (1.0.1)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=3.0.0->contextualSpellCheck) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.0.0->contextualSpellCheck) (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnhk00HipNi7",
        "outputId": "1e8db91d-72ee-4722-d011-14b9fe8bed5f"
      },
      "source": [
        "import spacy\n",
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-29 18:24:31.921279: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Requirement already satisfied: en-core-web-md==3.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.0.0/en_core_web_md-3.0.0-py3-none-any.whl#egg=en_core_web_md==3.0.0 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-md==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (54.1.2)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (20.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.4.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (8.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.1.1)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yN9pmsqpPqU"
      },
      "source": [
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import en_core_web_md\n",
        "import contextualSpellCheck\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.symbols import ORTH\n",
        "from spacy.tokenizer import _get_regex_pattern"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwVoKNI_pSv6"
      },
      "source": [
        "pd.set_option('display.max_rows', 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "JCNbepGjpZLg",
        "outputId": "35ddf50c-bb05-4b20-b024-71ea40d463f6"
      },
      "source": [
        "# Read in the dirty twitter data (with emojis cleaned from the previous file), store in a dataframe and display the first several rows. \n",
        "filename = \"/content/drive/MyDrive/Programming/Colab Notebooks/Coding_Dojo/Twitter_Sentiment_Project/intermediate_output_files/train_tweets_with_emojis_clean.csv\"\n",
        "\n",
        "tweet_df = pd.read_csv(filename, index_col=0)\n",
        "\n",
        "tweet_df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_emoji_cleaned</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>@user when a father is dysfunctional and is s...</td>\n",
              "      <td>@user when a father is dysfunctional and is s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
              "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>bihday your majesty</td>\n",
              "      <td>bihday your majesty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>#model   i love u take with u all the time in ...</td>\n",
              "      <td>#model   i love u take with u all the time in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
              "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>@user camping tomorrow @user @user @user @use...</td>\n",
              "      <td>@user camping tomorrow @user @user @user @use...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>the next school year is the year for exams.ð...</td>\n",
              "      <td>the next school year is the year for exams. su...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n",
              "      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>@user @user welcome here !  i'm   it's so #gr...</td>\n",
              "      <td>@user @user welcome here !  i'm   it's so #gr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    label  ...                                tweet_emoji_cleaned\n",
              "id         ...                                                   \n",
              "1       0  ...   @user when a father is dysfunctional and is s...\n",
              "2       0  ...  @user @user thanks for #lyft credit i can't us...\n",
              "3       0  ...                                bihday your majesty\n",
              "4       0  ...  #model   i love u take with u all the time in ...\n",
              "5       0  ...             factsguide: society now    #motivation\n",
              "6       0  ...  [2/2] huge fan fare and big talking before the...\n",
              "7       0  ...   @user camping tomorrow @user @user @user @use...\n",
              "8       0  ...  the next school year is the year for exams. su...\n",
              "9       0  ...  we won!!! love the land!!! #allin #cavs #champ...\n",
              "10      0  ...   @user @user welcome here !  i'm   it's so #gr...\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRCmcsO4sawX"
      },
      "source": [
        "# Part 1 - Load and setup spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmbut892r0oe",
        "outputId": "6b08874c-6ac6-4fcd-f2b5-a97ea62b6287"
      },
      "source": [
        "# Importing the trained Spacy model. \n",
        "nlp = en_core_web_md.load()\n",
        "\n",
        "contextualSpellCheck.add_to_pipe(nlp)\n",
        "\n",
        "nlp.pipe_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec',\n",
              " 'tagger',\n",
              " 'parser',\n",
              " 'ner',\n",
              " 'attribute_ruler',\n",
              " 'lemmatizer',\n",
              " 'contextual spellchecker']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "N4ha9BZbr5s2",
        "outputId": "4a88fe2b-a7ea-427b-a8cc-6b246592b16e"
      },
      "source": [
        "# Save off a single tweet to use when exploring tokenization behavior. \n",
        "first_tweet = tweet_df.loc[ tweet_df.index == 1, ['tweet_emoji_cleaned']]['tweet_emoji_cleaned'].to_numpy()\n",
        "\n",
        "first_tweet = str(first_tweet[0])\n",
        "\n",
        "first_tweet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu1rurlcr8PD"
      },
      "source": [
        "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# Remove \"#\" from the list of default prefixes spacy looks for during tokenization. This will allow us to add a token_match regular\n",
        "# expression that matches a hashtag and text (e.g. #hashTagsAreCool) as a single token. If '#' is not removed from the prefix list\n",
        "# then hashtags will always be a separate token from the text, this is because prefixes are processed before token_match rules during \n",
        "# tokenization. Reference the spacy tokenization chart for a visual of how rules are processed: https://spacy.io/usage/linguistic-features#tokenization\n",
        "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "default_prefixes = list(nlp.Defaults.prefixes)                      # The default prefixes spacy will look for during tokenization\n",
        "default_prefixes.remove('#')                                        # Remove hashtags for the default prefix list.\n",
        "prefix_regex = spacy.util.compile_prefix_regex(default_prefixes)    # Update to use the new prefixes\n",
        "nlp.tokenizer.prefix_search = prefix_regex.search\n",
        "\n",
        "# Get the current regex that nlp is using for token matching.\n",
        "nlp_token_matching_regex_pre_update = spacy.tokenizer._get_regex_pattern(nlp.tokenizer.token_match)\n",
        "\n",
        "# Create a new regex that combines the current regex and a term that will treat hashtags as a single token. \n",
        "updated_token_matching_regex = f\"({nlp_token_matching_regex_pre_update}|#\\w+)\"\n",
        "\n",
        "# Update the token matching regex used by nlp with the regex created in the line above.\n",
        "nlp.tokenizer.token_match = re.compile(updated_token_matching_regex).match\n",
        "\n",
        "# This is kept as an example of how to use nlp.tokenizer.explain to debug the tokenizer\n",
        "# tokenizer.explain tells you the rules being applied to get to the final result.\n",
        "'''\n",
        "tweet_doc = nlp.tokenizer.explain(first_tweet)   \n",
        "for token in tweet_doc: \n",
        "  print(token)\n",
        "'''\n",
        "\n",
        "# Turn the first_tweet string into a spacy doc object (sequence of tokens).\n",
        "tweet_doc = nlp(first_tweet)\n",
        "\n",
        "# Commented out but kept as an example of displaying Spacys behavior when turning \n",
        "# strings into doc objects (tokens). This output can also be used to verify that\n",
        "# the rule we created to keep hashtags as a single token worked as intended.\n",
        "'''\n",
        "print(f\"Token\\t\\tLemma\\t\\tStopword\")\n",
        "print(\"=\"*40)\n",
        "for token in tweet_doc:\n",
        "    print(f\"{token}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qztUHP2UsfuO"
      },
      "source": [
        "# Part 2 - Tweet cleaning with spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jcdBjGIsetW"
      },
      "source": [
        "#-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# This cell takes in a csv file that contains a list of common contractions, and their \"expanded\" counterparts. \n",
        "# The csv file is converted to a python dictionary, which will make it easy to lookup the expanded form of a word for any given contraction.\n",
        "# This dictionary will be used to convert all contractions to their expanded forms during the tweet cleaning process.\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "contraction_file = \"/content/drive/MyDrive/Programming/Colab Notebooks/Coding_Dojo/Twitter_Sentiment_Project/support_data/contractions.csv\"\n",
        "\n",
        "contractions_df = pd.read_csv(contraction_file)\n",
        "\n",
        "contractions = list(contractions_df['Contraction'].to_numpy())\n",
        "expanded_words = list(contractions_df['Expanded_Word'].to_numpy())\n",
        "\n",
        "contraction_to_expansion = zip(contractions, expanded_words)\n",
        "\n",
        "contraction_map = {}\n",
        "\n",
        "for index, mapping in enumerate(contraction_to_expansion):\n",
        "  contraction_map[mapping[0].lower()] = mapping[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIK-dpydslU2"
      },
      "source": [
        "#-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# \"SMS language\" is a term that refers to abbreviated or slang language that does not consist of proper dictionary words, but is nonetheless \n",
        "# commonly understood and widely used in digitial forms of communication. See wikipedia for more info on \"SMS language\" https://en.wikipedia.org/wiki/SMS_language\n",
        "#\n",
        "#-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "sms_speak_file = \"/content/drive/MyDrive/Programming/Colab Notebooks/Coding_Dojo/Twitter_Sentiment_Project/support_data/sms_speak.csv\"\n",
        "\n",
        "sms_speak_df = pd.read_csv(sms_speak_file)\n",
        "\n",
        "correct_wordings = list(sms_speak_df['Correct_Wording'].to_numpy())\n",
        "sms_abbreviations = list(sms_speak_df['SMS_Wording'].to_numpy())\n",
        "\n",
        "sms_and_correct_wordings = zip(sms_abbreviations, correct_wordings)\n",
        "\n",
        "sms_map = {}\n",
        "\n",
        "for index, mapping in enumerate(sms_and_correct_wordings):\n",
        "  sms_map[str(mapping[0]).lower()] = mapping[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCOF_Fnbsnsl"
      },
      "source": [
        "# --------------------------------------------------------------------------------------------------------------------------------------\n",
        "# Sometimes hashtags get stuck together, as in: #hashtagone#hastagtwo#hashtagthree. This function separates hashtags that are\n",
        "# stuck together so they get treated as their own entitiy.\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------\n",
        "def split_chained_hashtags(word):\n",
        "  \n",
        "  output_string = \"\" \n",
        "  if word.startswith('#') and word.count(\"#\") > 1:\n",
        "\n",
        "    individual_hashtags = word.split(\"#\")\n",
        "\n",
        "    for tag in individual_hashtags: \n",
        "\n",
        "      if tag != \"\":\n",
        "\n",
        "        output_string = output_string + \"#\" + tag + \" \"\n",
        "\n",
        "  else: \n",
        "    output_string = word\n",
        "\n",
        "  output_string = output_string.rstrip()\n",
        "  return output_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms0p-EjTsq53"
      },
      "source": [
        "# ---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# This is a helper function for the fix_contractions_punctuation_abbreviation function. When the parent function is ready to add a word to \n",
        "# the output tweet, this function helps by checking to see if there are any contractions or abbreviations we should use to transform the word first.\n",
        "# ---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "def evaluate_word(word, contraction_mapping=contraction_map, sms_mapping=sms_map):\n",
        "\n",
        "  output_word = \"\"\n",
        "\n",
        "  if word.lower() in contraction_mapping:                                         # Check if this word is a contraction that can be expanded. If so, add the expanded word to the output tweet.\n",
        "    output_word = output_word + \" \" + contraction_mapping[word.lower()]\n",
        "  elif word.lower() in sms_mapping:                                               # Check if this word is an abbreviation that can be expanded.\n",
        "    output_word = output_word + \" \" + sms_mapping[word.lower()]          \n",
        "  elif word != \"\":                                                        # If this word is not a contraction or abbreviation, and the word is not empty, add the word to the output tweet.\n",
        "    output_word = output_word + \" \" + word\n",
        "\n",
        "  if word.startswith('#') and word.count(\"#\") > 1:\n",
        "    output_word = split_chained_hashtags(word)\n",
        "\n",
        "  if word.count('\\'') >= 1: \n",
        "    output_word = output_word.replace('\\'', \"\")\n",
        "\n",
        "  return output_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEJuUbqossWP"
      },
      "source": [
        "def fix_contractions_punctuation_abbreviation(input_tweet, contraction_mapping=contraction_map, sms_mapping=sms_map):\n",
        "\n",
        "  punctuation = \".&\\()*+,-./:;<=>\\\"!?[\\\\]^_`{|}~\"\n",
        "  word = \"\" \n",
        "  output_tweet = \"\"\n",
        "\n",
        "  for char in input_tweet:                                                      # Iterate through every character in the tweet.\n",
        "\n",
        "    if char != \" \":                                                             # If we haven't hit a white space, continue on to keep building up a word.\n",
        "      if char in punctuation:                                                   # If we hit a punctuation mark, the current word is over.                                      \n",
        "        word = evaluate_word(word)                                              # Check if this word needs to be transformed to something else, or should be added as is.\n",
        "        output_tweet = output_tweet + \" \" + word                                # Add the appropriate word to the output tweet.\n",
        "        word = \"\"                                              \n",
        "      else:                                                                     # If this character was not a space or a punctuation mark, continue building up the word.\n",
        "        word = word + char\n",
        "    \n",
        "    else:                                                                       # If we reached the space before a new word starts, evaluate the word to see if it is a \n",
        "      word = evaluate_word(word)                                                # contraction or abbreviation. Add the appropriate word to the output tweet.\n",
        "      output_tweet = output_tweet + \" \" + word\n",
        "      word = \"\"\n",
        "\n",
        "  # If we are about to exit but still have one last word to add.\n",
        "  if word != \"\":                                                                # If the final character is not a whitespace, we may be ready to exit the function but still have one \n",
        "    word = evaluate_word(word)                                                  # remaining word to add to the output tweet. This section handles that scenario.\n",
        "    output_tweet = output_tweet + \" \" + word\n",
        "    word = \"\"\n",
        "\n",
        "  output_tweet = output_tweet.lstrip().rstrip()\n",
        "  output_tweet = re.sub(\" +\", \" \", output_tweet)\n",
        "\n",
        "  return output_tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7f4wUHc0xsX"
      },
      "source": [
        "### The next few cells contain helper functions for processing and selecting the desired spellcheck corrections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzSww5V_1DYs"
      },
      "source": [
        "global spelling_changes_made\n",
        "spelling_changes_made = {}\n",
        "\n",
        "global tweets_processed_for_spelling\n",
        "tweets_processed_for_spelling = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vl2htB7l1WbO"
      },
      "source": [
        "def record_changes_made(changes_being_made, original_tweet):\n",
        "\n",
        "  global tweets_processed_for_spelling\n",
        "  global spelling_changes_made\n",
        "\n",
        "  update_dict = {'original_tweet' : original_tweet,\n",
        "                 'original_word_spelling' : [],\n",
        "                 'new_spelling_after_contextualSpellCheck' : []}\n",
        "\n",
        "  for key, value in changes_being_made.items(): \n",
        "\n",
        "    update_dict['original_word_spelling'].append(key)\n",
        "    update_dict['new_spelling_after_contextualSpellCheck'].append(value)\n",
        "\n",
        "  spelling_changes_made[tweets_processed_for_spelling] = update_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf7QODa41E6l"
      },
      "source": [
        "def non_hashtag_changes(probabilities): \n",
        "\n",
        "  non_hashtag_spelling_changes = {}\n",
        "\n",
        "  for key, value in probabilities.items():\n",
        "\n",
        "    if str(key).startswith('#') == False: \n",
        "\n",
        "      non_hashtag_spelling_changes[key] = value\n",
        "\n",
        "  return non_hashtag_spelling_changes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVX51qIh1HB-"
      },
      "source": [
        "def keeper_changes(potential_changes, threshold=0.7):\n",
        "\n",
        "  changes_to_keep = {}\n",
        "\n",
        "  for key, value in potential_changes.items(): \n",
        "\n",
        "    best_change, best_probability = potential_changes[key][0]\n",
        "    #print(\"Potential_changes[key] as value: \", potential_changes[key])\n",
        "    #print(\"Potential_changes[key] as type: \", type(potential_changes[key]))\n",
        "\n",
        "    if best_probability >= threshold: \n",
        "      changes_to_keep[key] = best_change\n",
        "\n",
        "  return changes_to_keep"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC0tkd3m1UrB"
      },
      "source": [
        "def parse_changes(probabilities, doc, hash_check = True):\n",
        "  \n",
        "  if hash_check:                                                  # Only necessary to run the hash check if the parent function found hash tags were changed.\n",
        "    potential_changes = non_hashtag_changes(probabilities)        # Immediately ruling out any spelling change suggestions on hashtags. \n",
        "  else:\n",
        "    potential_changes = probabilities\n",
        "  \n",
        "  changes_to_implement = keeper_changes(potential_changes)        # Keeping changes where the probability of being correct is above a desired threshold. \n",
        "\n",
        "  original_string = str(doc)\n",
        "\n",
        "  if bool(changes_to_implement) == False:                         # Allow early exit in the case that there are no changes to implement.\n",
        "    #print(\"No changes to implement!\")\n",
        "    return original_string\n",
        "\n",
        "  record_changes_made(changes_to_implement, original_string)\n",
        "\n",
        "  original_str_as_tokens = doc\n",
        "\n",
        "  output_string = \"\"\n",
        "\n",
        "  for token in doc: \n",
        "\n",
        "    if token in changes_to_implement.keys():\n",
        "\n",
        "      output_string = output_string + \" \" + str(changes_to_implment[token]) + \" \"\n",
        "\n",
        "    else: \n",
        "\n",
        "      output_string = output_string + \" \" + str(token) + \" \"\n",
        "\n",
        "  output_string = output_string.lstrip().rstrip()\n",
        "  output_string = re.sub(\" +\", \" \", output_string)\n",
        "\n",
        "  #print(\"---------------------\")\n",
        "  #print(\"The original string was: \", original_string)\n",
        "  #print(\"The original string has type: \", type(original_string))\n",
        "  #print(\"The output string is: \", output_string)\n",
        "  #print(\"The original string has type: \", type(output_string))\n",
        "  #print(\"----------------------\\n\")\n",
        "\n",
        "  return output_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc9kiYCO1a6U"
      },
      "source": [
        "def spell_checker(doc):\n",
        "\n",
        "  global tweets_processed_for_spelling\n",
        "  tweets_processed_for_spelling = tweets_processed_for_spelling + 1\n",
        "\n",
        "  changed_hashtag = False\n",
        "  \n",
        "  if doc._.contextual_spellCheck:                     # Check if contextual spellcheck has been added as an extension. This should always pass.\n",
        "\n",
        "    if doc._.performed_spellCheck == True:            # If the spell checker identified one or more misspelled words.\n",
        "\n",
        "      probabilities = doc._.score_spellCheck          # Get a dictionary of the misspelled words, with their corrections and probabilities.\n",
        "\n",
        "      for key, value in probabilities.items():        # Check if any hashtags were corrected. This library is terrible at spell checking hash tags. \n",
        "        #print(key, value)\n",
        "        #print(\"\\n\")\n",
        "        if str(key).startswith('#') == True:\n",
        "          changed_hashtag = True\n",
        "\n",
        "      if changed_hashtag == True: \n",
        "        string_with_corrections = parse_changes(probabilities, doc)\n",
        "        return string_with_corrections\n",
        "      else: \n",
        "        string_with_corrections = parse_changes(probabilities, doc, hash_check=False)\n",
        "        return string_with_corrections\n",
        "  \n",
        "  return doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59zj3PyEtgOK"
      },
      "source": [
        "# --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# Tweet Cleaner Function. When applied to a tweet, this function will do the following:\n",
        "#\n",
        "# - Remove punctuation marks.\n",
        "# - Convert common contractions and abbreviations to their expanded forms.\n",
        "# - Convert the tweet to a Spacy doc object (a set of tokens).\n",
        "# - Use Spacys lemmatization process to group words together and reduce the overall number of unique tokens\n",
        "#   Note: Reducing the number of unique tokens is desired because that will help reduce the dimensionality of the matrix we use when building models, \n",
        "#         and will therefore help mitigate the negative effects of working in high dimensional spaces (curse of dimensionality).\n",
        "#\n",
        "# - Convert all words to lowercase, to further reduce the number of unique words in the output.\n",
        "# - Remove all stop words as defined in Spacys list of default stop words. Use nlp.Defaults.stop_words for a list.\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def tweet_cleaner(dirty_tweet, nlp):\n",
        "\n",
        "  partially_clean_tweet = fix_contractions_punctuation_abbreviation(dirty_tweet)\n",
        "  stop_words = nlp.Defaults.stop_words\n",
        "  partially_clean_tweet_doc = nlp(partially_clean_tweet)\n",
        "\n",
        "  spell_check_result = spell_checker(partially_clean_tweet_doc)\n",
        "\n",
        "  try: \n",
        "    tweet_doc = nlp(spell_check_result)\n",
        "  except TypeError:\n",
        "    tweet_doc = spell_check_result\n",
        "\n",
        "  clean_tweet_tokens = []\n",
        "  for token in tweet_doc:\n",
        "    if token.lemma_ == \"-PRON-\":                                # -PRON- indicates that Spacys lemmatizer flagged this work as a generic pronoun.\n",
        "      token_txt = token.lemma_.lower().strip('-')               # If the lemma of the token is -PRON- strip off the \"-\"'s and make the word lowercase.\n",
        "      clean_tweet_tokens.append(token_txt)  \n",
        "    \n",
        "    # Ignore @users tokens and stopwords. They are generic and are unlikely to help provide insight.\n",
        "    elif token.lemma_ != '@user' and token.lemma_ not in stop_words and token.lemma_ != \"\" and token.lemma_ != \" \":\n",
        "      token_txt = token.lemma_.lower().strip()               \n",
        "      clean_tweet_tokens.append(token_txt)\n",
        "  \n",
        "  return clean_tweet_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOLQBmdp1e8L"
      },
      "source": [
        "### The cell below creates a test driver so we can try the spell check functionality on a smaller number of datapoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6T9YeDU1z9c"
      },
      "source": [
        "def spell_check_test_drive(test_df=tweet_df, model=nlp, test_start_index=1, test_end_index=31): \n",
        "\n",
        "  test_start = str(test_start_index)\n",
        "  test_end = str(test_end_index)\n",
        "\n",
        "  global spelling_changes_made\n",
        "  spelling_changes_made = {}\n",
        "\n",
        "  global tweets_processed_for_spelling\n",
        "  tweets_processed_for_spelling = 0\n",
        "\n",
        "  test_tweets = list(test_df.iloc[test_start_index:test_end_index, 2].to_numpy())\n",
        "\n",
        "  test_result_tokens = []\n",
        "\n",
        "  for tweet in test_tweets: \n",
        "\n",
        "    result = tweet_cleaner(tweet, model)\n",
        "    test_result_tokens.append(result)\n",
        "\n",
        "  PATH = \"/content/drive/MyDrive/Programming/Colab Notebooks/Coding_Dojo/Twitter_Sentiment_Project/troubleshooting/\"\n",
        "  filename = test_start + \"_to_\" + test_end + \"_spellcheck_test_results.csv\"\n",
        "  filepath = PATH + filename\n",
        "\n",
        "  test_spelling_changes_made_df = pd.DataFrame(spelling_changes_made).T\n",
        "\n",
        "  test_spelling_changes_made_df.to_csv(path_or_buf=filepath)\n",
        "\n",
        "  return test_result_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_8bWTQuD6JX"
      },
      "source": [
        "test = spell_check_test_drive(test_start_index=200, test_end_index=250)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h79yAs1JtlqX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "cf696d30-e00a-4344-efff-50be03c09def"
      },
      "source": [
        "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# Creates a new column \"clean\", tokenzied tweet.\n",
        "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "'''\n",
        "tweet_df['Fully_Clean_Tweet_Tokenized'] = tweet_df['tweet_emoji_cleaned'].apply(tweet_cleaner, args=(nlp,))\n",
        "\n",
        "tweet_df.to_csv(path_or_buf=\"/content/drive/MyDrive/Programming/Colab Notebooks/Coding_Dojo/Twitter_Sentiment_Project/intermediate_output_files/train_tweets_spacy_clean.csv\")\n",
        "\n",
        "tweet_df.head()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ntweet_df[\\'Fully_Clean_Tweet_Tokenized\\'] = tweet_df[\\'tweet_emoji_cleaned\\'].apply(tweet_cleaner, args=(nlp,))\\n\\ntweet_df.to_csv(path_or_buf=\"/content/drive/MyDrive/Programming/Colab Notebooks/Coding_Dojo/Twitter_Sentiment_Project/intermediate_output_files/train_tweets_spacy_clean.csv\")\\n\\ntweet_df.head()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_Data_Cleaning_Current_Spellcheck_Issue.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMkn3MoxcYiHF/EmLD3fFzp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BradenAnderson/Twitter-Sentiment-Analysis/blob/main/01_Data_Cleaning_Current_Spellcheck_Issue.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODAVsDq8he10"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI9Eh8-HpkvC"
      },
      "source": [
        "!pip install contextualSpellCheck"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmL8A2O9iW3r"
      },
      "source": [
        "import spacy\n",
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPbi_B9diXWN"
      },
      "source": [
        "import pickle\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import en_core_web_md\n",
        "import contextualSpellCheck\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.symbols import ORTH\n",
        "from spacy.tokenizer import _get_regex_pattern\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, SCORERS, multilabel_confusion_matrix, silhouette_score\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import NearestNeighbors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XFUwO_7mIz7"
      },
      "source": [
        "pd.set_option('display.max_rows', 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lj4Bj3Q2iZo3"
      },
      "source": [
        "# Read in the dirty twitter data, store in a dataframe and display the first several rows. \n",
        "\n",
        "filename = '/content/drive/MyDrive/Programming/Colab Notebooks/Coding_Dojo/Twitter_Sentiment_Project/train_twitter_sentiment.csv'\n",
        "\n",
        "tweet_df = pd.read_csv(filename, index_col=0)\n",
        "\n",
        "emoji_file = '/content/drive/MyDrive/Programming/Colab Notebooks/Coding_Dojo/Twitter_Sentiment_Project/emoji_partial.csv'\n",
        "\n",
        "emoji_df = pd.read_csv(emoji_file)\n",
        "\n",
        "pd.set_option('display.max_rows', 1000)\n",
        "\n",
        "tweet_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6x-MUcQjhkc"
      },
      "source": [
        "emoji_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56aUQyYOyqQz"
      },
      "source": [
        "# Part 1 - Replace Emojis\n",
        "\n",
        "Emojis are replaced with text that describes the sentiment of the emoji if possible. If the sentiment for that particular emoji is unknown, the emoji is replaced with an empty string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9hT2UOKkEDY"
      },
      "source": [
        "def codePointToLatinUnicode(code_point):\n",
        "\n",
        "  output_string = \"\"\n",
        "\n",
        "  for char in code_point: \n",
        "\n",
        "    if char != \"U\" and char != \"+\":\n",
        "\n",
        "      output_string = output_string + char\n",
        "\n",
        "  output = int(output_string, 16) # Covert the output string to 16 bit hex\n",
        "\n",
        "  output = chr(output) \n",
        "\n",
        "  output = output.encode('utf-8').decode('latin-1')\n",
        "\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cgaJZa_lpWg"
      },
      "source": [
        "emoji_df['Unicode'] = emoji_df['Codepoint'].apply(codePointToLatinUnicode)\n",
        "\n",
        "emoji_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlzCJ1QVmqkc"
      },
      "source": [
        "code_point_values = list(emoji_df.loc[: , 'Codepoint'].to_numpy())\n",
        "\n",
        "latin_unicode_values = []\n",
        "\n",
        "for code in code_point_values: \n",
        "\n",
        "  value = codePointToLatinUnicode(code)\n",
        "\n",
        "  latin_unicode_values.append(value)\n",
        "\n",
        "emoji_sentiments = list(emoji_df.loc[:, 'Sentiment'].to_numpy())\n",
        "code_to_sentiment = zip(latin_unicode_values, emoji_sentiments)\n",
        "\n",
        "emoji_map = {}\n",
        "\n",
        "for index, mapping in enumerate(code_to_sentiment): \n",
        "\n",
        "  emoji_map[mapping[0]] = mapping[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnpv87SClqUL"
      },
      "source": [
        "# This global variable is used to generate a list of all emojis that I currently do not have a sentiment for.\n",
        "global unknown_emoji_list\n",
        "unknown_emoji_list = []\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------\n",
        "# Uses the emoji_map dictionary to convert the emoji unicode representation to the desired sentiment.\n",
        "# Some extra logic is included for handling situations where an emoji is not in the emoji_map dictionary\n",
        "# or situations where multiple emojis are stuck together and therefore there codes run together as one.\n",
        "#----------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def getEmojiSentiment(emoji_code): \n",
        "\n",
        "  global unknown_emoji_list\n",
        "\n",
        "  sentiment = \"\"\n",
        "\n",
        "  if emoji_code in emoji_map.keys(): \n",
        "    sentiment = \" \" + emoji_map[emoji_code] + \" \"\n",
        "    return sentiment\n",
        "  \n",
        "  else: \n",
        "\n",
        "    code = \"\"\n",
        "\n",
        "    for char in emoji_code:\n",
        "\n",
        "      code = code + char\n",
        "\n",
        "      if code in emoji_map.keys():\n",
        "        sentiment = \" \" + sentiment + \" \" + emoji_map[code] + \" \"\n",
        "        code = \"\"\n",
        "\n",
        "\n",
        "    if sentiment == \"\":\n",
        "\n",
        "      unknown_emoji_list.append(emoji_code)\n",
        "\n",
        "    return sentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC8odr0UqnOe"
      },
      "source": [
        "def convertEmojiToSentiment(input_tweet): \n",
        "\n",
        "  building_emoji_code = False\n",
        "  emoji_code = \"\"\n",
        "  output_tweet = \"\"\n",
        "\n",
        "  for char in input_tweet:  \n",
        "\n",
        "    if ord(char) > 127 and building_emoji_code == False:          # If this character is the first character in a new emoji.\n",
        "\n",
        "      building_emoji_code = True                                  # Indicate that we are now building an emoji code.\n",
        "      emoji_code = emoji_code + char                              # Store the first character in the emoji code.\n",
        "  \n",
        "    elif ord(char) > 127 and building_emoji_code == True:         # If this character is part of an emoji, and we are already in the middle of building an emoji code.\n",
        "\n",
        "      emoji_code = emoji_code + char                              # Continue building the emoji code, this will be used to get the emoji sentiment once the code is complete.\n",
        "\n",
        "    elif ord(char) < 127 and building_emoji_code == True:         # If we were just building an emoji code, but this character is not part of an emoji.\n",
        "\n",
        "      emoji_description = getEmojiSentiment(emoji_code)           # The emoji code is now complete, go get the sentiment for that emoji.\n",
        "\n",
        "      output_tweet = output_tweet + emoji_description             # Add the emojis sentiment to the output tweet.\n",
        "\n",
        "      emoji_code = \"\"                                             # Reset the emoji code and sentiment to prepare to store the next one.\n",
        "      emoji_description = \"\"\n",
        "\n",
        "      building_emoji_code = False                                 # Reset flag indicating we are no longer building an emoji. \n",
        "\n",
        "      output_tweet = output_tweet + char                          # This character was not part of an emoji, so pass it through to the output tweet.\n",
        "\n",
        "    elif ord(char) < 127 and building_emoji_code == False:        # If this character is not part of an emoji, and we have not been building one. \n",
        "\n",
        "      output_tweet = output_tweet + char                          # This character was not part of an emoji, so pass it through to the output tweet.\n",
        "\n",
        "  if len(emoji_code) != 0:                                        # This section handles a situation where an emoji code was the final character in a tweet.\n",
        "\n",
        "    emoji_description = getEmojiSentiment(emoji_code)             # Go get the sentiment for this emoji.\n",
        "\n",
        "    output_tweet = output_tweet + emoji_description               # Add the emojis sentiment to the output tweet.\n",
        "\n",
        "  return output_tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN0wsEtUsq5W"
      },
      "source": [
        "# ------------------------------------------------------------------------------------------------------------------------------------\n",
        "# Creates a new column in tweet_df where all emojis are removed. If a sentiment for that emojis exists in the emoji_map dictionary,\n",
        "# then the sentiment is added in place of the emoji. If no sentiment exists, the emoji is replaced with an empty string.\n",
        "# ------------------------------------------------------------------------------------------------------------------------------------\n",
        "tweet_df['tweet_emoji_cleaned'] = tweet_df['tweet'].apply(convertEmojiToSentiment)\n",
        "\n",
        "tweet_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z58Hg2KNtbEj"
      },
      "source": [
        "unknown_emojis = list(set(unknown_emoji_list))\n",
        "\n",
        "unlisted_emoji_dict = {'Code_From_Tweet' : [], 'Code_Converted' : []}\n",
        "\n",
        "for emoji in unknown_emojis:\n",
        "\n",
        "  unlisted_emoji_dict['Code_From_Tweet'].append(emoji)\n",
        "\n",
        "  try: \n",
        "\n",
        "    unlisted_emoji_dict['Code_Converted'].append(emoji.encode('latin-1').decode('utf-8'))\n",
        "\n",
        "  except UnicodeDecodeError:\n",
        "\n",
        "    unlisted_emoji_dict['Code_Converted'].append(\"Code could not be converted.\")\n",
        "\n",
        "emojis_no_sentiment_df = pd.DataFrame(unlisted_emoji_dict)\n",
        "\n",
        "emojis_no_sentiment_df.head()\n",
        "\n",
        "emojis_no_sentiment_df.to_csv(path_or_buf='/content/drive/MyDrive/Programming/Colab Notebooks/Coding_Dojo/Twitter_Sentiment_Project/unknown_emoji.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg9K0376zvh2"
      },
      "source": [
        "# Part 2 - Setting up Spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agHd6eDA2sIg"
      },
      "source": [
        "# Importing the trained Spacy model. \n",
        "nlp = en_core_web_md.load()\n",
        "\n",
        "contextualSpellCheck.add_to_pipe(nlp)\n",
        "\n",
        "nlp.pipe_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHCG_X5A5i3L"
      },
      "source": [
        "# Save off a single tweet to use when exploring tokenization behavior. \n",
        "first_tweet = tweet_df.loc[ tweet_df.index == 1, ['tweet_emoji_cleaned']]['tweet_emoji_cleaned'].to_numpy()\n",
        "\n",
        "first_tweet = str(first_tweet[0])\n",
        "\n",
        "first_tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fChxKd_sF9Z"
      },
      "source": [
        "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# Remove \"#\" from the list of default prefixes spacy looks for during tokenization. This will allow us to add a token_match regular\n",
        "# expression that matches a hashtag and text (e.g. #hashTagsAreCool) as a single token. If '#' is not removed from the prefix list\n",
        "# then hashtags will always be a separate token from the text, this is because prefixes are processed before token_match rules during \n",
        "# tokenization. Reference the spacy tokenization chart for a visual of how rules are processed: https://spacy.io/usage/linguistic-features#tokenization\n",
        "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "default_prefixes = list(nlp.Defaults.prefixes)                      # The default prefixes spacy will look for during tokenization\n",
        "default_prefixes.remove('#')                                        # Remove hashtags for the default prefix list.\n",
        "prefix_regex = spacy.util.compile_prefix_regex(default_prefixes)    # Update to use the new prefixes\n",
        "nlp.tokenizer.prefix_search = prefix_regex.search\n",
        "\n",
        "# Get the current regex that nlp is using for token matching.\n",
        "nlp_token_matching_regex_pre_update = spacy.tokenizer._get_regex_pattern(nlp.tokenizer.token_match)\n",
        "\n",
        "# Create a new regex that combines the current regex and a term that will treat hashtags as a single token. \n",
        "updated_token_matching_regex = f\"({nlp_token_matching_regex_pre_update}|#\\w+)\"\n",
        "\n",
        "# Update the token matching regex used by nlp with the regex created in the line above.\n",
        "nlp.tokenizer.token_match = re.compile(updated_token_matching_regex).match\n",
        "\n",
        "# This is kept as an example of how to use nlp.tokenizer.explain to debug the tokenizer\n",
        "# tokenizer.explain tells you the rules being applied to get to the final result.\n",
        "'''\n",
        "tweet_doc = nlp.tokenizer.explain(first_tweet)   \n",
        "for token in tweet_doc: \n",
        "  print(token)\n",
        "'''\n",
        "\n",
        "# Turn the first_tweet string into a spacy doc object (sequence of tokens).\n",
        "tweet_doc = nlp(first_tweet)\n",
        "\n",
        "# Commented out but kept as an example of displaying Spacys behavior when turning \n",
        "# strings into doc objects (tokens). This output can also be used to verify that\n",
        "# the rule we created to keep hashtags as a single token worked as intended.\n",
        "'''\n",
        "print(f\"Token\\t\\tLemma\\t\\tStopword\")\n",
        "print(\"=\"*40)\n",
        "for token in tweet_doc:\n",
        "    print(f\"{token}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-xfQ17L9uw2"
      },
      "source": [
        "# Part 3 - Tweet Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CViYgZ2a_DPl"
      },
      "source": [
        "#-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# This cell takes in a csv file that contains a list of common contractions, and their \"expanded\" counterparts. \n",
        "# The csv file is converted to a python dictionary, which will make it easy to lookup the expanded form of a word for any given contraction.\n",
        "# This dictionary will be used to convert all contractions to their expanded forms during the tweet cleaning process.\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "contraction_file = '/content/drive/MyDrive/Programming/Colab Notebooks/Coding_Dojo/Twitter_Sentiment_Project/contractions.csv'\n",
        "\n",
        "contractions_df = pd.read_csv(contraction_file)\n",
        "\n",
        "contractions = list(contractions_df['Contraction'].to_numpy())\n",
        "expanded_words = list(contractions_df['Expanded_Word'].to_numpy())\n",
        "\n",
        "contraction_to_expansion = zip(contractions, expanded_words)\n",
        "\n",
        "contraction_map = {}\n",
        "\n",
        "for index, mapping in enumerate(contraction_to_expansion):\n",
        "  contraction_map[mapping[0].lower()] = mapping[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOJiZIeZBNEY"
      },
      "source": [
        "#-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# \"SMS language\" is a term that refers to abbreviated or slang language that does not consist of proper dictionary words, but is nonetheless \n",
        "# commonly understood and widely used in digitial forms of communication. See wikipedia for more info on \"SMS language\" https://en.wikipedia.org/wiki/SMS_language\n",
        "#\n",
        "#\n",
        "#-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "sms_speak_file = '/content/drive/MyDrive/Programming/Colab Notebooks/Coding_Dojo/Twitter_Sentiment_Project/sms_speak.csv'\n",
        "\n",
        "sms_speak_df = pd.read_csv(sms_speak_file)\n",
        "\n",
        "correct_wordings = list(sms_speak_df['Correct_Wording'].to_numpy())\n",
        "sms_abbreviations = list(sms_speak_df['SMS_Wording'].to_numpy())\n",
        "\n",
        "sms_and_correct_wordings = zip(sms_abbreviations, correct_wordings)\n",
        "\n",
        "sms_map = {}\n",
        "\n",
        "for index, mapping in enumerate(sms_and_correct_wordings):\n",
        "  sms_map[str(mapping[0]).lower()] = mapping[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsMR5hSNJEer"
      },
      "source": [
        "# --------------------------------------------------------------------------------------------------------------------------------------\n",
        "# Sometimes hashtags get stuck together, as in: #hashtagone#hastagtwo#hashtagthree. This function separates hashtags that are\n",
        "# stuck together so they get treated as their own entitiy.\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------\n",
        "def split_chained_hashtags(word):\n",
        "  \n",
        "  output_string = \"\" \n",
        "  if word.startswith('#') and word.count(\"#\") > 1:\n",
        "\n",
        "    individual_hashtags = word.split(\"#\")\n",
        "\n",
        "    for tag in individual_hashtags: \n",
        "\n",
        "      if tag != \"\":\n",
        "\n",
        "        output_string = output_string + \"#\" + tag + \" \"\n",
        "\n",
        "  else: \n",
        "    output_string = word\n",
        "\n",
        "  output_string = output_string.rstrip()\n",
        "  return output_string\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU9d2TCbMcw1"
      },
      "source": [
        "# ---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# This is a helper function for the fix_contractions_punctuation_abbreviation function. When the parent function is ready to add a word to \n",
        "# the output tweet, this function helps by checking to see if there are any contractions or abbreviations we should use to transform the word first.\n",
        "# ---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "def evaluate_word(word, contraction_mapping=contraction_map, sms_mapping=sms_map):\n",
        "\n",
        "  output_word = \"\"\n",
        "\n",
        "  if word.lower() in contraction_mapping:                                         # Check if this word is a contraction that can be expanded. If so, add the expanded word to the output tweet.\n",
        "    output_word = output_word + \" \" + contraction_mapping[word.lower()]\n",
        "  elif word.lower() in sms_mapping:                                               # Check if this word is an abbreviation that can be expanded.\n",
        "    output_word = output_word + \" \" + sms_mapping[word.lower()]          \n",
        "  elif word != \"\":                                                        # If this word is not a contraction or abbreviation, and the word is not empty, add the word to the output tweet.\n",
        "    output_word = output_word + \" \" + word\n",
        "\n",
        "  if word.startswith('#') and word.count(\"#\") > 1:\n",
        "    output_word = split_chained_hashtags(word)\n",
        "\n",
        "  if word.count('\\'') >= 1: \n",
        "    output_word = output_word.replace('\\'', \"\")\n",
        "\n",
        "  return output_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBe3HAm3FUiu"
      },
      "source": [
        "def fix_contractions_punctuation_abbreviation(input_tweet, contraction_mapping=contraction_map, sms_mapping=sms_map):\n",
        "\n",
        "  punctuation = \".&\\()*+,-./:;<=>\\\"!?[\\\\]^_`{|}~\"\n",
        "  word = \"\" \n",
        "  output_tweet = \"\"\n",
        "\n",
        "  for char in input_tweet:                                                      # Iterate through every character in the tweet.\n",
        "\n",
        "    if char != \" \":                                                             # If we haven't hit a white space, continue on to keep building up a word.\n",
        "      if char in punctuation:                                                   # If we hit a punctuation mark, the current word is over.                                      \n",
        "        word = evaluate_word(word)                                              # Check if this word needs to be transformed to something else, or should be added as is.\n",
        "        output_tweet = output_tweet + \" \" + word                                # Add the appropriate word to the output tweet.\n",
        "        word = \"\"                                              \n",
        "      else:                                                                     # If this character was not a space or a punctuation mark, continue building up the word.\n",
        "        word = word + char\n",
        "    \n",
        "    else:                                                                       # If we reached the space before a new word starts, evaluate the word to see if it is a \n",
        "      word = evaluate_word(word)                                                # contraction or abbreviation. Add the appropriate word to the output tweet.\n",
        "      output_tweet = output_tweet + \" \" + word\n",
        "      word = \"\"\n",
        "\n",
        "  # If we are about to exit but still have one last word to add.\n",
        "  if word != \"\":                                                                # If the final character is not a whitespace, we may be ready to exit the function but still have one \n",
        "    word = evaluate_word(word)                                                  # remaining word to add to the output tweet. This section handles that scenario.\n",
        "    output_tweet = output_tweet + \" \" + word\n",
        "    word = \"\"\n",
        "\n",
        "  output_tweet = output_tweet.lstrip().rstrip()\n",
        "  output_tweet = re.sub(\" +\", \" \", output_tweet)\n",
        "\n",
        "  return output_tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X694OqstnnoA"
      },
      "source": [
        "#filter = (tweet_df.index == 17) & (tweet_df.index == 18) & (tweet_df.index == 19) & (tweet_df.index == 20) & (tweet_df.index == 21)\n",
        "\n",
        "#test = tweet_df.iloc[100:150, 2].to_numpy()\n",
        "\n",
        "#test;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj-xYnH1Jp5J"
      },
      "source": [
        "# test = list(test);\n",
        "\n",
        "# test;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzATR3AcB42S"
      },
      "source": [
        "'''\n",
        "output_list = []\n",
        "\n",
        "for item in test:\n",
        "\n",
        "  func_test = fix_contractions_punctuation_abbreviation(item)\n",
        "\n",
        "  output_list.append(func_test)\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57jao3ksJ5E4"
      },
      "source": [
        "# output_list;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46K2j3WP-3Eq"
      },
      "source": [
        "'''\n",
        "doc_list = []\n",
        "\n",
        "for item in output_list: \n",
        "\n",
        "  test_doc = nlp(item)\n",
        "  doc_list.append(test_doc)\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmlHr8l-i5YW"
      },
      "source": [
        "global spelling_changes_made\n",
        "spelling_changes_made = {}\n",
        "\n",
        "global tweets_processed_for_spelling\n",
        "tweets_processed_for_spelling = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaxni2p8VBBK"
      },
      "source": [
        "def non_hashtag_changes(probabilities): \n",
        "\n",
        "  non_hashtag_spelling_changes = {}\n",
        "\n",
        "  for key, value in probabilities.items():\n",
        "\n",
        "    if str(key).startswith('#') == False: \n",
        "\n",
        "      non_hashtag_spelling_changes[key] = value\n",
        "\n",
        "  return non_hashtag_spelling_changes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4gusJD2WXqw"
      },
      "source": [
        "def keeper_changes(potential_changes, threshold=0.7):\n",
        "\n",
        "  changes_to_keep = {}\n",
        "\n",
        "  for key, value in potential_changes.items(): \n",
        "\n",
        "    best_change, best_probability = potential_changes[key][0]\n",
        "    #print(\"Potential_changes[key] as value: \", potential_changes[key])\n",
        "    #print(\"Potential_changes[key] as type: \", type(potential_changes[key]))\n",
        "\n",
        "    if best_probability >= threshold: \n",
        "      changes_to_keep[key] = best_change\n",
        "\n",
        "  return changes_to_keep\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oDMv-4vjk0e"
      },
      "source": [
        "def record_changes_made(changes_being_made, original_tweet):\n",
        "\n",
        "  global tweets_processed_for_spelling\n",
        "  global spelling_changes_made\n",
        "\n",
        "  update_dict = {'original_tweet' : original_tweet,\n",
        "                 'original_word_spelling' : [],\n",
        "                 'new_spelling_after_contextualSpellCheck' : []}\n",
        "\n",
        "  for key, value in changes_being_made.items(): \n",
        "\n",
        "    update_dict['original_word_spelling'].append(key)\n",
        "    update_dict['new_spelling_after_contextualSpellCheck'].append(value)\n",
        "\n",
        "  spelling_changes_made[tweets_processed_for_spelling] = update_dict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DDtkOEVSNSu"
      },
      "source": [
        "def parse_changes(probabilities, doc, hash_check = True):\n",
        "  \n",
        "  if hash_check:                                                  # Only necessary to run the hash check if the parent function found hash tags were changed.\n",
        "    potential_changes = non_hashtag_changes(probabilities)        # Immediately ruling out any spelling change suggestions on hashtags. \n",
        "  else:\n",
        "    potential_changes = probabilities\n",
        "  \n",
        "  changes_to_implement = keeper_changes(potential_changes)        # Keeping changes where the probability of being correct is above a desired threshold. \n",
        "\n",
        "  original_string = str(doc)\n",
        "\n",
        "  if bool(changes_to_implement) == False:                         # Allow early exit in the case that there are no changes to implement.\n",
        "    #print(\"No changes to implement!\")\n",
        "    return original_string\n",
        "\n",
        "  record_changes_made(changes_to_implement, original_string)\n",
        "\n",
        "  original_str_as_tokens = doc\n",
        "\n",
        "  output_string = \"\"\n",
        "\n",
        "  for token in doc: \n",
        "\n",
        "    if token in changes_to_implement.keys():\n",
        "\n",
        "      output_string = output_string + \" \" + str(changes_to_implment[token]) + \" \"\n",
        "\n",
        "    else: \n",
        "\n",
        "      output_string = output_string + \" \" + str(token) + \" \"\n",
        "\n",
        "  output_string = output_string.lstrip().rstrip()\n",
        "  output_string = re.sub(\" +\", \" \", output_string)\n",
        "\n",
        "  #print(\"---------------------\")\n",
        "  #print(\"The original string was: \", original_string)\n",
        "  #print(\"The original string has type: \", type(original_string))\n",
        "  #print(\"The output string is: \", output_string)\n",
        "  #print(\"The original string has type: \", type(output_string))\n",
        "  #print(\"----------------------\\n\")\n",
        "\n",
        "  return output_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuXXPECAtasG"
      },
      "source": [
        "def spell_checker(doc):\n",
        "\n",
        "  global tweets_processed_for_spelling\n",
        "  tweets_processed_for_spelling = tweets_processed_for_spelling + 1\n",
        "\n",
        "  changed_hashtag = False\n",
        "  \n",
        "  if doc._.contextual_spellCheck:                     # Check if contextual spellcheck has been added as an extension. This should always pass.\n",
        "\n",
        "    if doc._.performed_spellCheck == True:            # If the spell checker identified one or more misspelled words.\n",
        "\n",
        "      probabilities = doc._.score_spellCheck          # Get a dictionary of the misspelled words, with their corrections and probabilities.\n",
        "\n",
        "      for key, value in probabilities.items():        # Check if any hashtags were corrected. This library is terrible at spell checking hash tags. \n",
        "        #print(key, value)\n",
        "        #print(\"\\n\")\n",
        "        if str(key).startswith('#') == True:\n",
        "          changed_hashtag = True\n",
        "\n",
        "      if changed_hashtag == True: \n",
        "        string_with_corrections = parse_changes(probabilities, doc)\n",
        "        return string_with_corrections\n",
        "      else: \n",
        "        string_with_corrections = parse_changes(probabilities, doc, hash_check=False)\n",
        "        return string_with_corrections\n",
        "     \n",
        "  return doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogrwgoqQKtpf"
      },
      "source": [
        "'''first_doc = doc_list[6]\n",
        "\n",
        "first_check = spell_checker(first_doc)\n",
        "\n",
        "type(first_check)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjUKbFiUI4hb"
      },
      "source": [
        "'''\n",
        "spelling_checked_docs = []\n",
        "loop_count = 0\n",
        "for doc_obj in doc_list:\n",
        "  spell_check_result = spell_checker(doc_obj)\n",
        "\n",
        "  loop_count = loop_count + 1\n",
        "  print(\"--------Loop \", str(loop_count), \"----------------\")\n",
        "\n",
        "  try: \n",
        "    spelling_corrected_doc = nlp(spell_check_result)\n",
        "    spelling_checked_docs.append((spelling_corrected_doc, 'Changed'))\n",
        "  except TypeError:\n",
        "    spelling_corrected_doc = spell_check_result\n",
        "    spelling_checked_docs.append((spelling_corrected_doc, 'Original'))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek8kniAhpsx5"
      },
      "source": [
        "#spelling_changes_made;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj_239L9MXIB"
      },
      "source": [
        "#spelling_checked_docs;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbwbIOJtMfJX"
      },
      "source": [
        "#doc_list;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O891jnqgvME2"
      },
      "source": [
        "#out = spell_checker(test_doc);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHCGb7M3FncC"
      },
      "source": [
        "#type(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB6D-Wfd6REv"
      },
      "source": [
        "# --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# Tweet Cleaner Function. When applied to a tweet, this function will do the following:\n",
        "#\n",
        "# - Remove punctuation marks.\n",
        "# - Convert common contractions and abbreviations to their expanded forms.\n",
        "# - Convert the tweet to a Spacy doc object (a set of tokens).\n",
        "# - Use Spacys lemmatization process to group words together and reduce the overall number of unique tokens\n",
        "#   Note: Reducing the number of unique tokens is desired because that will help reduce the dimensionality of the matrix we use when building models, \n",
        "#         and will therefore help mitigate the negative effects of working in high dimensional spaces (curse of dimensionality).\n",
        "#\n",
        "# - Convert all words to lowercase, to further reduce the number of unique words in the output.\n",
        "# - Remove all stop words as defined in Spacys list of default stop words. Use nlp.Defaults.stop_words for a list.\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def tweet_cleaner(dirty_tweet, nlp):\n",
        "\n",
        "  partially_clean_tweet = fix_contractions_punctuation_abbreviation(dirty_tweet)\n",
        "  stop_words = nlp.Defaults.stop_words\n",
        "  partially_clean_tweet_doc = nlp(partially_clean_tweet)\n",
        "\n",
        "  spell_check_result = spell_checker(partially_clean_tweet_doc)\n",
        "  \n",
        "  try: \n",
        "    tweet_doc = nlp(spell_check_result)\n",
        "  except TypeError:\n",
        "    tweet_doc = spell_check_result\n",
        "\n",
        "  clean_tweet_tokens = []\n",
        "  for token in tweet_doc:\n",
        "    if token.lemma_ == \"-PRON-\":                                # -PRON- indicates that Spacys lemmatizer flagged this work as a generic pronoun.\n",
        "      token_txt = token.lemma_.lower().strip('-')               # If the lemma of the token is -PRON- strip off the \"-\"'s and make the word lowercase.\n",
        "      clean_tweet_tokens.append(token_txt)  \n",
        "    \n",
        "    # Ignore @users tokens and stopwords. They are generic and are unlikely to help provide insight.\n",
        "    elif token.lemma_ != '@user' and token.lemma_ not in stop_words and token.lemma_ != \"\" and token.lemma_ != \" \":\n",
        "      token_txt = token.lemma_.lower().strip()               \n",
        "      clean_tweet_tokens.append(token_txt)\n",
        "  \n",
        "  return clean_tweet_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFpDc-Rx9hsU"
      },
      "source": [
        "tweet_df['Fully_Clean_Tweet_Tokenized'] = tweet_df['tweet_emoji_cleaned'].apply(tweet_cleaner, args=(nlp,))\n",
        "\n",
        "filepath = '/content/drive/MyDrive/Programming/Colab Notebooks/Coding_Dojo/Twitter_Sentiment_Project/tweet_df_incorrect_spellcheck.csv'\n",
        "tweet_df.to_csv(path_or_buff=filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUmC5YcU-M0K"
      },
      "source": [
        "tweet_df.head(30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEuXkRPPt2SD"
      },
      "source": [
        "changes_df = pd.DataFrame(spelling_changes_made).T\n",
        "\n",
        "filepath = '/content/drive/MyDrive/Programming/Colab Notebooks/Coding_Dojo/Twitter_Sentiment_Project/Contextual_SpellCheck_Incorrect_Output.csv'\n",
        "\n",
        "changes_df.to_csv(path_or_buff=filepath)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5l5BMvZw33g"
      },
      "source": [
        "changes_df.head(50)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_Data_Cleaning_With_Spacy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOWRFaKyVnWJ+9+IjfZTJvl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BradenAnderson/Twitter-Sentiment-Analysis/blob/main/01_Data_Cleaning_With_Spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOvcOUW_o5oh",
        "outputId": "118b0dee-6323-4806-96f1-8f6d0f152ea9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnhk00HipNi7",
        "outputId": "4ca05573-70ad-42e3-88e5-0e6662f676e4"
      },
      "source": [
        "import spacy\n",
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_md==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz#egg=en_core_web_md==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (54.1.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.7.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yN9pmsqpPqU"
      },
      "source": [
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import en_core_web_md\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.symbols import ORTH\n",
        "from spacy.tokenizer import _get_regex_pattern"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwVoKNI_pSv6"
      },
      "source": [
        "pd.set_option('display.max_rows', 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "JCNbepGjpZLg",
        "outputId": "e69f6589-a234-4d06-fa39-e87216cd7f85"
      },
      "source": [
        "# Read in the dirty twitter data (with emojis cleaned from the previous file), store in a dataframe and display the first several rows. \n",
        "filename = \"/content/drive/MyDrive/Programming/Colab Notebooks/Coding_Dojo/Twitter_Sentiment_Project/intermediate_output_files/train_tweets_with_emojis_clean.csv\"\n",
        "\n",
        "tweet_df = pd.read_csv(filename, index_col=0)\n",
        "\n",
        "tweet_df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_emoji_cleaned</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>@user when a father is dysfunctional and is s...</td>\n",
              "      <td>@user when a father is dysfunctional and is s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
              "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>bihday your majesty</td>\n",
              "      <td>bihday your majesty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>#model   i love u take with u all the time in ...</td>\n",
              "      <td>#model   i love u take with u all the time in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
              "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>@user camping tomorrow @user @user @user @use...</td>\n",
              "      <td>@user camping tomorrow @user @user @user @use...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>the next school year is the year for exams.ð...</td>\n",
              "      <td>the next school year is the year for exams. su...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n",
              "      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>@user @user welcome here !  i'm   it's so #gr...</td>\n",
              "      <td>@user @user welcome here !  i'm   it's so #gr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    label  ...                                tweet_emoji_cleaned\n",
              "id         ...                                                   \n",
              "1       0  ...   @user when a father is dysfunctional and is s...\n",
              "2       0  ...  @user @user thanks for #lyft credit i can't us...\n",
              "3       0  ...                                bihday your majesty\n",
              "4       0  ...  #model   i love u take with u all the time in ...\n",
              "5       0  ...             factsguide: society now    #motivation\n",
              "6       0  ...  [2/2] huge fan fare and big talking before the...\n",
              "7       0  ...   @user camping tomorrow @user @user @user @use...\n",
              "8       0  ...  the next school year is the year for exams. su...\n",
              "9       0  ...  we won!!! love the land!!! #allin #cavs #champ...\n",
              "10      0  ...   @user @user welcome here !  i'm   it's so #gr...\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRCmcsO4sawX"
      },
      "source": [
        "# Part 1 - Load and setup spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmbut892r0oe",
        "outputId": "cc3295f7-3056-48fb-a16c-d0095e670b99"
      },
      "source": [
        "# Importing the trained Spacy model. \n",
        "nlp = en_core_web_md.load()\n",
        "\n",
        "nlp.pipe_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tagger', 'parser', 'ner']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "N4ha9BZbr5s2",
        "outputId": "1159459d-89cd-4054-ea61-80bdeb504717"
      },
      "source": [
        "# Save off a single tweet to use when exploring tokenization behavior. \n",
        "first_tweet = tweet_df.loc[ tweet_df.index == 1, ['tweet_emoji_cleaned']]['tweet_emoji_cleaned'].to_numpy()\n",
        "\n",
        "first_tweet = str(first_tweet[0])\n",
        "\n",
        "first_tweet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu1rurlcr8PD"
      },
      "source": [
        "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# Remove \"#\" from the list of default prefixes spacy looks for during tokenization. This will allow us to add a token_match regular\n",
        "# expression that matches a hashtag and text (e.g. #hashTagsAreCool) as a single token. If '#' is not removed from the prefix list\n",
        "# then hashtags will always be a separate token from the text, this is because prefixes are processed before token_match rules during \n",
        "# tokenization. Reference the spacy tokenization chart for a visual of how rules are processed: https://spacy.io/usage/linguistic-features#tokenization\n",
        "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "default_prefixes = list(nlp.Defaults.prefixes)                      # The default prefixes spacy will look for during tokenization\n",
        "default_prefixes.remove('#')                                        # Remove hashtags for the default prefix list.\n",
        "prefix_regex = spacy.util.compile_prefix_regex(default_prefixes)    # Update to use the new prefixes\n",
        "nlp.tokenizer.prefix_search = prefix_regex.search\n",
        "\n",
        "# Get the current regex that nlp is using for token matching.\n",
        "nlp_token_matching_regex_pre_update = spacy.tokenizer._get_regex_pattern(nlp.tokenizer.token_match)\n",
        "\n",
        "# Create a new regex that combines the current regex and a term that will treat hashtags as a single token. \n",
        "updated_token_matching_regex = f\"({nlp_token_matching_regex_pre_update}|#\\w+)\"\n",
        "\n",
        "# Update the token matching regex used by nlp with the regex created in the line above.\n",
        "nlp.tokenizer.token_match = re.compile(updated_token_matching_regex).match\n",
        "\n",
        "# This is kept as an example of how to use nlp.tokenizer.explain to debug the tokenizer\n",
        "# tokenizer.explain tells you the rules being applied to get to the final result.\n",
        "'''\n",
        "tweet_doc = nlp.tokenizer.explain(first_tweet)   \n",
        "for token in tweet_doc: \n",
        "  print(token)\n",
        "'''\n",
        "\n",
        "# Turn the first_tweet string into a spacy doc object (sequence of tokens).\n",
        "tweet_doc = nlp(first_tweet)\n",
        "\n",
        "# Commented out but kept as an example of displaying Spacys behavior when turning \n",
        "# strings into doc objects (tokens). This output can also be used to verify that\n",
        "# the rule we created to keep hashtags as a single token worked as intended.\n",
        "'''\n",
        "print(f\"Token\\t\\tLemma\\t\\tStopword\")\n",
        "print(\"=\"*40)\n",
        "for token in tweet_doc:\n",
        "    print(f\"{token}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qztUHP2UsfuO"
      },
      "source": [
        "# Part 2 - Tweet cleaning with spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jcdBjGIsetW"
      },
      "source": [
        "#-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# This cell takes in a csv file that contains a list of common contractions, and their \"expanded\" counterparts. \n",
        "# The csv file is converted to a python dictionary, which will make it easy to lookup the expanded form of a word for any given contraction.\n",
        "# This dictionary will be used to convert all contractions to their expanded forms during the tweet cleaning process.\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "contraction_file = \"/content/drive/MyDrive/Programming/Colab Notebooks/Coding_Dojo/Twitter_Sentiment_Project/support_data/contractions.csv\"\n",
        "\n",
        "contractions_df = pd.read_csv(contraction_file)\n",
        "\n",
        "contractions = list(contractions_df['Contraction'].to_numpy())\n",
        "expanded_words = list(contractions_df['Expanded_Word'].to_numpy())\n",
        "\n",
        "contraction_to_expansion = zip(contractions, expanded_words)\n",
        "\n",
        "contraction_map = {}\n",
        "\n",
        "for index, mapping in enumerate(contraction_to_expansion):\n",
        "  contraction_map[mapping[0].lower()] = mapping[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIK-dpydslU2"
      },
      "source": [
        "#-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# \"SMS language\" is a term that refers to abbreviated or slang language that does not consist of proper dictionary words, but is nonetheless \n",
        "# commonly understood and widely used in digitial forms of communication. See wikipedia for more info on \"SMS language\" https://en.wikipedia.org/wiki/SMS_language\n",
        "#\n",
        "#-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "sms_speak_file = \"/content/drive/MyDrive/Programming/Colab Notebooks/Coding_Dojo/Twitter_Sentiment_Project/support_data/sms_speak.csv\"\n",
        "\n",
        "sms_speak_df = pd.read_csv(sms_speak_file)\n",
        "\n",
        "correct_wordings = list(sms_speak_df['Correct_Wording'].to_numpy())\n",
        "sms_abbreviations = list(sms_speak_df['SMS_Wording'].to_numpy())\n",
        "\n",
        "sms_and_correct_wordings = zip(sms_abbreviations, correct_wordings)\n",
        "\n",
        "sms_map = {}\n",
        "\n",
        "for index, mapping in enumerate(sms_and_correct_wordings):\n",
        "  sms_map[str(mapping[0]).lower()] = mapping[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCOF_Fnbsnsl"
      },
      "source": [
        "# --------------------------------------------------------------------------------------------------------------------------------------\n",
        "# Sometimes hashtags get stuck together, as in: #hashtagone#hastagtwo#hashtagthree. This function separates hashtags that are\n",
        "# stuck together so they get treated as their own entitiy.\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------\n",
        "def split_chained_hashtags(word):\n",
        "  \n",
        "  output_string = \"\" \n",
        "  if word.startswith('#') and word.count(\"#\") > 1:\n",
        "\n",
        "    individual_hashtags = word.split(\"#\")\n",
        "\n",
        "    for tag in individual_hashtags: \n",
        "\n",
        "      if tag != \"\":\n",
        "\n",
        "        output_string = output_string + \"#\" + tag + \" \"\n",
        "\n",
        "  else: \n",
        "    output_string = word\n",
        "\n",
        "  output_string = output_string.rstrip()\n",
        "  return output_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms0p-EjTsq53"
      },
      "source": [
        "# ---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# This is a helper function for the fix_contractions_punctuation_abbreviation function. When the parent function is ready to add a word to \n",
        "# the output tweet, this function helps by checking to see if there are any contractions or abbreviations we should use to transform the word first.\n",
        "# ---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "def evaluate_word(word, contraction_mapping=contraction_map, sms_mapping=sms_map):\n",
        "\n",
        "  output_word = \"\"\n",
        "\n",
        "  if word.lower() in contraction_mapping:                                         # Check if this word is a contraction that can be expanded. If so, add the expanded word to the output tweet.\n",
        "    output_word = output_word + \" \" + contraction_mapping[word.lower()]\n",
        "  elif word.lower() in sms_mapping:                                               # Check if this word is an abbreviation that can be expanded.\n",
        "    output_word = output_word + \" \" + sms_mapping[word.lower()]          \n",
        "  elif word != \"\":                                                        # If this word is not a contraction or abbreviation, and the word is not empty, add the word to the output tweet.\n",
        "    output_word = output_word + \" \" + word\n",
        "\n",
        "  if word.startswith('#') and word.count(\"#\") > 1:\n",
        "    output_word = split_chained_hashtags(word)\n",
        "\n",
        "  if word.count('\\'') >= 1: \n",
        "    output_word = output_word.replace('\\'', \"\")\n",
        "\n",
        "  return output_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEJuUbqossWP"
      },
      "source": [
        "def fix_contractions_punctuation_abbreviation(input_tweet, contraction_mapping=contraction_map, sms_mapping=sms_map):\n",
        "\n",
        "  punctuation = \".&\\()*+,-./:;<=>\\\"!?[\\\\]^_`{|}~\"\n",
        "  word = \"\" \n",
        "  output_tweet = \"\"\n",
        "\n",
        "  for char in input_tweet:                                                      # Iterate through every character in the tweet.\n",
        "\n",
        "    if char != \" \":                                                             # If we haven't hit a white space, continue on to keep building up a word.\n",
        "      if char in punctuation:                                                   # If we hit a punctuation mark, the current word is over.                                      \n",
        "        word = evaluate_word(word)                                              # Check if this word needs to be transformed to something else, or should be added as is.\n",
        "        output_tweet = output_tweet + \" \" + word                                # Add the appropriate word to the output tweet.\n",
        "        word = \"\"                                              \n",
        "      else:                                                                     # If this character was not a space or a punctuation mark, continue building up the word.\n",
        "        word = word + char\n",
        "    \n",
        "    else:                                                                       # If we reached the space before a new word starts, evaluate the word to see if it is a \n",
        "      word = evaluate_word(word)                                                # contraction or abbreviation. Add the appropriate word to the output tweet.\n",
        "      output_tweet = output_tweet + \" \" + word\n",
        "      word = \"\"\n",
        "\n",
        "  # If we are about to exit but still have one last word to add.\n",
        "  if word != \"\":                                                                # If the final character is not a whitespace, we may be ready to exit the function but still have one \n",
        "    word = evaluate_word(word)                                                  # remaining word to add to the output tweet. This section handles that scenario.\n",
        "    output_tweet = output_tweet + \" \" + word\n",
        "    word = \"\"\n",
        "\n",
        "  output_tweet = output_tweet.lstrip().rstrip()\n",
        "  output_tweet = re.sub(\" +\", \" \", output_tweet)\n",
        "\n",
        "  return output_tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59zj3PyEtgOK"
      },
      "source": [
        "# --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# Tweet Cleaner Function. When applied to a tweet, this function will do the following:\n",
        "#\n",
        "# - Remove punctuation marks.\n",
        "# - Convert common contractions and abbreviations to their expanded forms.\n",
        "# - Convert the tweet to a Spacy doc object (a set of tokens).\n",
        "# - Use Spacys lemmatization process to group words together and reduce the overall number of unique tokens\n",
        "#   Note: Reducing the number of unique tokens is desired because that will help reduce the dimensionality of the matrix we use when building models, \n",
        "#         and will therefore help mitigate the negative effects of working in high dimensional spaces (curse of dimensionality).\n",
        "#\n",
        "# - Convert all words to lowercase, to further reduce the number of unique words in the output.\n",
        "# - Remove all stop words as defined in Spacys list of default stop words. Use nlp.Defaults.stop_words for a list.\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def tweet_cleaner(dirty_tweet, nlp):\n",
        "\n",
        "  partially_clean_tweet = fix_contractions_punctuation_abbreviation(dirty_tweet)\n",
        "  stop_words = nlp.Defaults.stop_words\n",
        "\n",
        "  tweet_doc = nlp(partially_clean_tweet)\n",
        "\n",
        "  clean_tweet_tokens = []\n",
        "  for token in tweet_doc:\n",
        "    if token.lemma_ == \"-PRON-\":                                # -PRON- indicates that Spacys lemmatizer flagged this work as a generic pronoun.\n",
        "      token_txt = token.lemma_.lower().strip('-')               # If the lemma of the token is -PRON- strip off the \"-\"'s and make the word lowercase.\n",
        "      clean_tweet_tokens.append(token_txt)  \n",
        "    \n",
        "    # Ignore @users tokens and stopwords. They are generic and are unlikely to help provide insight.\n",
        "    elif token.lemma_ != '@user' and token.lemma_ not in stop_words and token.lemma_ != \"\" and token.lemma_ != \" \":\n",
        "      token_txt = token.lemma_.lower().strip()               \n",
        "      clean_tweet_tokens.append(token_txt)\n",
        "  \n",
        "  return clean_tweet_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "h79yAs1JtlqX",
        "outputId": "62c42689-477b-4d45-f9d7-18281e42a477"
      },
      "source": [
        "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# Creates a new column \"clean\", tokenzied tweet.\n",
        "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "tweet_df['Fully_Clean_Tweet_Tokenized'] = tweet_df['tweet_emoji_cleaned'].apply(tweet_cleaner, args=(nlp,))\n",
        "\n",
        "tweet_df.to_csv(path_or_buf=\"/content/drive/MyDrive/Programming/Colab Notebooks/Coding_Dojo/Twitter_Sentiment_Project/intermediate_output_files/train_tweets_spacy_clean.csv\")\n",
        "\n",
        "tweet_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_emoji_cleaned</th>\n",
              "      <th>Fully_Clean_Tweet_Tokenized</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>@user when a father is dysfunctional and is s...</td>\n",
              "      <td>@user when a father is dysfunctional and is s...</td>\n",
              "      <td>[father, dysfunctional, significant, selfish, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
              "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
              "      <td>[thank, #lyft, credit, use, cause, pron, offer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>bihday your majesty</td>\n",
              "      <td>bihday your majesty</td>\n",
              "      <td>[bihday, pron, majesty]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>#model   i love u take with u all the time in ...</td>\n",
              "      <td>#model   i love u take with u all the time in ...</td>\n",
              "      <td>[#model, love, pron, pron, time, pron, happy, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "      <td>[factsguide, society, #motivation]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    label  ...                        Fully_Clean_Tweet_Tokenized\n",
              "id         ...                                                   \n",
              "1       0  ...  [father, dysfunctional, significant, selfish, ...\n",
              "2       0  ...  [thank, #lyft, credit, use, cause, pron, offer...\n",
              "3       0  ...                            [bihday, pron, majesty]\n",
              "4       0  ...  [#model, love, pron, pron, time, pron, happy, ...\n",
              "5       0  ...                 [factsguide, society, #motivation]\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 846
        },
        "id": "-UjpDiic0TGX",
        "outputId": "0889a8b7-aef4-41aa-f6f3-e2a0835d3a8d"
      },
      "source": [
        "tweet_df.head(25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_emoji_cleaned</th>\n",
              "      <th>Fully_Clean_Tweet_Tokenized</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>@user when a father is dysfunctional and is s...</td>\n",
              "      <td>@user when a father is dysfunctional and is s...</td>\n",
              "      <td>[father, dysfunctional, significant, selfish, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
              "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
              "      <td>[thank, #lyft, credit, use, cause, pron, offer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>bihday your majesty</td>\n",
              "      <td>bihday your majesty</td>\n",
              "      <td>[bihday, pron, majesty]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>#model   i love u take with u all the time in ...</td>\n",
              "      <td>#model   i love u take with u all the time in ...</td>\n",
              "      <td>[#model, love, pron, pron, time, pron, happy, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "      <td>[factsguide, society, #motivation]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
              "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
              "      <td>[2, 2, huge, fan, fare, big, talking, pron, le...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>@user camping tomorrow @user @user @user @use...</td>\n",
              "      <td>@user camping tomorrow @user @user @user @use...</td>\n",
              "      <td>[camping, tomorrow, danny]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>the next school year is the year for exams.ð...</td>\n",
              "      <td>the next school year is the year for exams. su...</td>\n",
              "      <td>[school, year, year, exam, surprise, amazed, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n",
              "      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n",
              "      <td>[pron, win, love, land, #allin, #cavs, #champi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>@user @user welcome here !  i'm   it's so #gr...</td>\n",
              "      <td>@user @user welcome here !  i'm   it's so #gr...</td>\n",
              "      <td>[welcome, pron, pron, significant, #gr8]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>â #ireland consumer price index (mom) climb...</td>\n",
              "      <td>#ireland consumer price index (mom) climbed ...</td>\n",
              "      <td>[#ireland, consumer, price, index, mom, climb,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>we are so selfish. #orlando #standwithorlando ...</td>\n",
              "      <td>we are so selfish. #orlando #standwithorlando ...</td>\n",
              "      <td>[pron, significant, selfish, #orlando, #standw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>i get to see my daddy today!!   #80days #getti...</td>\n",
              "      <td>i get to see my daddy today!!   #80days #getti...</td>\n",
              "      <td>[pron, daddy, today, #80days, #gettingfed]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1</td>\n",
              "      <td>@user #cnn calls #michigan middle school 'buil...</td>\n",
              "      <td>@user #cnn calls #michigan middle school 'buil...</td>\n",
              "      <td>[#cnn, #michigan, middle, school, build, wall,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>no comment!  in #australia   #opkillingbay #se...</td>\n",
              "      <td>no comment!  in #australia   #opkillingbay #se...</td>\n",
              "      <td>[comment, #australia, #opkillingbay, #seasheph...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0</td>\n",
              "      <td>ouch...junior is angryð#got7 #junior #yugyo...</td>\n",
              "      <td>ouch...junior is angry irritation concern #got...</td>\n",
              "      <td>[ouch, junior, angry, irritation, concern, #go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>i am thankful for having a paner. #thankful #p...</td>\n",
              "      <td>i am thankful for having a paner. #thankful #p...</td>\n",
              "      <td>[thankful, paner, #thankful, #positive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1</td>\n",
              "      <td>retweet if you agree!</td>\n",
              "      <td>retweet if you agree!</td>\n",
              "      <td>[retweet, pron, agree]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>its #friday! ð smiles all around via ig use...</td>\n",
              "      <td>its #friday!  happy  smiles all around via ig ...</td>\n",
              "      <td>[pron, #friday, happy, smile, instagram, user,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0</td>\n",
              "      <td>as we all know, essential oils are not made of...</td>\n",
              "      <td>as we all know, essential oils are not made of...</td>\n",
              "      <td>[pron, know, essential, oil, chemical]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0</td>\n",
              "      <td>#euro2016 people blaming ha for conceded goal ...</td>\n",
              "      <td>#euro2016 people blaming ha for conceded goal ...</td>\n",
              "      <td>[#euro2016, people, blame, ha, conceded, goal,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0</td>\n",
              "      <td>sad little dude..   #badday #coneofshame #cats...</td>\n",
              "      <td>sad little dude..   #badday #coneofshame #cats...</td>\n",
              "      <td>[sad, little, dude, #badday, #coneofshame, #ca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0</td>\n",
              "      <td>product of the day: happy man #wine tool  who'...</td>\n",
              "      <td>product of the day: happy man #wine tool  who'...</td>\n",
              "      <td>[product, day, happy, man, #wine, tool, pron, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1</td>\n",
              "      <td>@user @user lumpy says i am a . prove it lumpy.</td>\n",
              "      <td>@user @user lumpy says i am a . prove it lumpy.</td>\n",
              "      <td>[lumpy, prove, pron, lumpy]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0</td>\n",
              "      <td>@user #tgif   #ff to my #gamedev #indiedev #i...</td>\n",
              "      <td>@user #tgif   #ff to my #gamedev #indiedev #i...</td>\n",
              "      <td>[#tgif, #ff, pron, #gamedev, #indiedev, #indie...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    label  ...                        Fully_Clean_Tweet_Tokenized\n",
              "id         ...                                                   \n",
              "1       0  ...  [father, dysfunctional, significant, selfish, ...\n",
              "2       0  ...  [thank, #lyft, credit, use, cause, pron, offer...\n",
              "3       0  ...                            [bihday, pron, majesty]\n",
              "4       0  ...  [#model, love, pron, pron, time, pron, happy, ...\n",
              "5       0  ...                 [factsguide, society, #motivation]\n",
              "6       0  ...  [2, 2, huge, fan, fare, big, talking, pron, le...\n",
              "7       0  ...                         [camping, tomorrow, danny]\n",
              "8       0  ...  [school, year, year, exam, surprise, amazed, t...\n",
              "9       0  ...  [pron, win, love, land, #allin, #cavs, #champi...\n",
              "10      0  ...           [welcome, pron, pron, significant, #gr8]\n",
              "11      0  ...  [#ireland, consumer, price, index, mom, climb,...\n",
              "12      0  ...  [pron, significant, selfish, #orlando, #standw...\n",
              "13      0  ...         [pron, daddy, today, #80days, #gettingfed]\n",
              "14      1  ...  [#cnn, #michigan, middle, school, build, wall,...\n",
              "15      1  ...  [comment, #australia, #opkillingbay, #seasheph...\n",
              "16      0  ...  [ouch, junior, angry, irritation, concern, #go...\n",
              "17      0  ...            [thankful, paner, #thankful, #positive]\n",
              "18      1  ...                             [retweet, pron, agree]\n",
              "19      0  ...  [pron, #friday, happy, smile, instagram, user,...\n",
              "20      0  ...             [pron, know, essential, oil, chemical]\n",
              "21      0  ...  [#euro2016, people, blame, ha, conceded, goal,...\n",
              "22      0  ...  [sad, little, dude, #badday, #coneofshame, #ca...\n",
              "23      0  ...  [product, day, happy, man, #wine, tool, pron, ...\n",
              "24      1  ...                        [lumpy, prove, pron, lumpy]\n",
              "25      0  ...  [#tgif, #ff, pron, #gamedev, #indiedev, #indie...\n",
              "\n",
              "[25 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}
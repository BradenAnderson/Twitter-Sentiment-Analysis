{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_Data_Cleaning_With_Spacy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMLH5CQ39/SUMbj1KIOgxBl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BradenAnderson/Twitter-Sentiment-Analysis/blob/main/01_Data_Cleaning_With_Spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOvcOUW_o5oh",
        "outputId": "71f062d9-f108-4769-af27-21f06d820fa0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnhk00HipNi7"
      },
      "source": [
        "import spacy\n",
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yN9pmsqpPqU"
      },
      "source": [
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import en_core_web_md\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.symbols import ORTH\n",
        "from spacy.tokenizer import _get_regex_pattern"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwVoKNI_pSv6"
      },
      "source": [
        "pd.set_option('display.max_rows', 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "JCNbepGjpZLg",
        "outputId": "5026489f-b416-4067-82a6-ba3c1adafc1b"
      },
      "source": [
        "# Read in the tweet dataset output by the 00_Emoji_Data_Cleaning notebook. Emojis are now clean but the rest of the tweet has not be preprocessed yet.\n",
        "filename = \"/content/drive/MyDrive/Programming/Colab Notebooks/Coding_Dojo/Twitter_Sentiment_Project/intermediate_output_files/train_tweets_with_emojis_clean.csv\"\n",
        "\n",
        "tweet_df = pd.read_csv(filename, index_col=0)\n",
        "\n",
        "tweet_df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_emoji_cleaned</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>@user when a father is dysfunctional and is s...</td>\n",
              "      <td>@user when a father is dysfunctional and is s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
              "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>bihday your majesty</td>\n",
              "      <td>bihday your majesty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>#model   i love u take with u all the time in ...</td>\n",
              "      <td>#model   i love u take with u all the time in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
              "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>@user camping tomorrow @user @user @user @use...</td>\n",
              "      <td>@user camping tomorrow @user @user @user @use...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>the next school year is the year for exams.ð...</td>\n",
              "      <td>the next school year is the year for exams. su...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n",
              "      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>@user @user welcome here !  i'm   it's so #gr...</td>\n",
              "      <td>@user @user welcome here !  i'm   it's so #gr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    label  ...                                tweet_emoji_cleaned\n",
              "id         ...                                                   \n",
              "1       0  ...   @user when a father is dysfunctional and is s...\n",
              "2       0  ...  @user @user thanks for #lyft credit i can't us...\n",
              "3       0  ...                                bihday your majesty\n",
              "4       0  ...  #model   i love u take with u all the time in ...\n",
              "5       0  ...             factsguide: society now    #motivation\n",
              "6       0  ...  [2/2] huge fan fare and big talking before the...\n",
              "7       0  ...   @user camping tomorrow @user @user @user @use...\n",
              "8       0  ...  the next school year is the year for exams. su...\n",
              "9       0  ...  we won!!! love the land!!! #allin #cavs #champ...\n",
              "10      0  ...   @user @user welcome here !  i'm   it's so #gr...\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRCmcsO4sawX"
      },
      "source": [
        "# Part 1 - Load and setup spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmbut892r0oe",
        "outputId": "7e122328-2eaf-489d-d57d-13af7c038b1b"
      },
      "source": [
        "# Importing the trained spaCy model. \n",
        "nlp = en_core_web_md.load()\n",
        "\n",
        "# View the standard pipeline components that SpaCy uses for this model.\n",
        "nlp.pipe_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tagger', 'parser', 'ner']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "N4ha9BZbr5s2",
        "outputId": "fc01b563-249d-4224-ee75-10fae2c237d7"
      },
      "source": [
        "# Save off a single tweet to use when exploring tokenization behavior. \n",
        "first_tweet = tweet_df.loc[ tweet_df.index == 1, ['tweet_emoji_cleaned']]['tweet_emoji_cleaned'].to_numpy()\n",
        "\n",
        "first_tweet = str(first_tweet[0])\n",
        "\n",
        "first_tweet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu1rurlcr8PD"
      },
      "source": [
        "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# Remove \"#\" from the list of default prefixes spaCy looks for during tokenization. This will allow us to create a token_match regular\n",
        "# expression that matches a hashtag and text (e.g. #hashTagsAreCool) as a single token. \n",
        "#\n",
        "# If '#' is not removed from the prefix list then hashtags will always be a separate token from the text, regardless of whether or not we have a token\n",
        "# match rule that would match a #text style string. This is because spaCy processes prefix rules before token_match rules during tokenization process. \n",
        "#\n",
        "# Reference the spacy tokenization chart for a visual of how rules are processed: https://spacy.io/usage/linguistic-features#tokenization\n",
        "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# The following four lines of code remove the # symbol from the prefix rules.\n",
        "\n",
        "default_prefixes = list(nlp.Defaults.prefixes)                      # The default prefixes spacy will look for during tokenization\n",
        "default_prefixes.remove('#')                                        # Remove hashtags from the default prefix list\n",
        "prefix_regex = spacy.util.compile_prefix_regex(default_prefixes)    # Compile the new prefix regex (that now has # removed)\n",
        "nlp.tokenizer.prefix_search = prefix_regex.search                   # Update the \"nlp\" trained model to use the new prefix regex\n",
        "\n",
        "\n",
        "# The next three lines add a regular expression that will match '#text' as a single token to the token_match regex.\n",
        "\n",
        "# Get the current regex that nlp is using for token matching.\n",
        "nlp_token_matching_regex_pre_update = spacy.tokenizer._get_regex_pattern(nlp.tokenizer.token_match)\n",
        "\n",
        "# Create a new regex that combines the current regex and a term that will treat hashtags as a single token. \n",
        "updated_token_matching_regex = f\"({nlp_token_matching_regex_pre_update}|#\\w+)\"\n",
        "\n",
        "# Update the token matching regex used by nlp to the regex created in the line above.\n",
        "nlp.tokenizer.token_match = re.compile(updated_token_matching_regex).match\n",
        "\n",
        "# This is kept as an example of how to use nlp.tokenizer.explain to debug the tokenizer\n",
        "# tokenizer.explain tells you the rules being applied to get to the final tokenized result.\n",
        "'''\n",
        "tweet_doc = nlp.tokenizer.explain(first_tweet)   \n",
        "for token in tweet_doc: \n",
        "  print(token)\n",
        "'''\n",
        "\n",
        "# Commented out but kept as an example of displaying spaCys behavior when turning \n",
        "# strings into doc objects (tokens). This output can also be used to verify that\n",
        "# the rule we created to keep hashtags as a single token worked as intended.\n",
        "'''\n",
        "\n",
        "# Turn the first_tweet string into a spacy doc object (sequence of tokens).\n",
        "tweet_doc = nlp(first_tweet)\n",
        "\n",
        "print(f\"Token\\t\\tLemma\\t\\tStopword\")\n",
        "print(\"=\"*40)\n",
        "for token in tweet_doc:\n",
        "    print(f\"{token}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qztUHP2UsfuO"
      },
      "source": [
        "# Part 2 - Tweet cleaning with spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jcdBjGIsetW"
      },
      "source": [
        "#-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# This cell takes in a csv file that contains a list of common contractions, and their \"expanded\" counterparts. \n",
        "# The csv file is converted to a python dictionary, which will make it easy to lookup the expanded form of a word for any given contraction.\n",
        "# This dictionary will be used to convert all contractions to their expanded forms during the tweet cleaning process.\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "contraction_file = \"/content/drive/MyDrive/Programming/Colab Notebooks/Coding_Dojo/Twitter_Sentiment_Project/support_data/contractions.csv\"\n",
        "\n",
        "contractions_df = pd.read_csv(contraction_file)\n",
        "\n",
        "contractions = list(contractions_df['Contraction'].to_numpy())\n",
        "expanded_words = list(contractions_df['Expanded_Word'].to_numpy())\n",
        "\n",
        "contraction_to_expansion = zip(contractions, expanded_words)\n",
        "\n",
        "contraction_map = {}\n",
        "\n",
        "for index, mapping in enumerate(contraction_to_expansion):\n",
        "  contraction_map[mapping[0].lower()] = mapping[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIK-dpydslU2"
      },
      "source": [
        "#-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# \"SMS language\" is a term that refers to abbreviated or slang language that does not consist of proper dictionary words, but is nonetheless \n",
        "# commonly understood and widely used in digitial forms of communication. See wikipedia for more info on \"SMS language\" https://en.wikipedia.org/wiki/SMS_language\n",
        "#\n",
        "# This cell takes in a csv file that contains a list of common \"SMS laguage\" and the corresponding \"expanded\" form. For example SMS speak '<3' would be converted to 'love'. A \n",
        "# dictionary is created that maps the sms term the the associated 'correct wording'. \n",
        "#-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "sms_speak_file = \"/content/drive/MyDrive/Programming/Colab Notebooks/Coding_Dojo/Twitter_Sentiment_Project/support_data/sms_speak.csv\"\n",
        "\n",
        "sms_speak_df = pd.read_csv(sms_speak_file)\n",
        "\n",
        "correct_wordings = list(sms_speak_df['Correct_Wording'].to_numpy())\n",
        "sms_abbreviations = list(sms_speak_df['SMS_Wording'].to_numpy())\n",
        "\n",
        "sms_and_correct_wordings = zip(sms_abbreviations, correct_wordings)\n",
        "\n",
        "sms_map = {}\n",
        "\n",
        "for index, mapping in enumerate(sms_and_correct_wordings):\n",
        "  sms_map[str(mapping[0]).lower()] = mapping[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCOF_Fnbsnsl"
      },
      "source": [
        "# --------------------------------------------------------------------------------------------------------------------------------------\n",
        "# Sometimes hashtags get stuck together, as in: #hashtagone#hastagtwo#hashtagthree. This function separates hashtags that are\n",
        "# stuck together so they get treated as their own entitiy.\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------\n",
        "def split_chained_hashtags(word):\n",
        "  \n",
        "  output_string = \"\" \n",
        "  if word.startswith('#') and word.count(\"#\") > 1:                              # If there is more than one # symbol in the word.\n",
        "\n",
        "    individual_hashtags = word.split(\"#\")                                       # Split the word into multiple words at each # symbol. \n",
        "\n",
        "    for tag in individual_hashtags: \n",
        "\n",
        "      if tag != \"\":\n",
        "\n",
        "        output_string = output_string + \"#\" + tag + \" \"                         # Output string consists of each individual hashtag (#text) separated by whitespace (e.g. #hashtag1 #hashtag2)\n",
        "\n",
        "  else: \n",
        "    output_string = word\n",
        "\n",
        "  output_string = output_string.rstrip()\n",
        "  return output_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms0p-EjTsq53"
      },
      "source": [
        "# ---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# This is a helper function for the fix_contractions_punctuation_abbreviation function.\n",
        "#\n",
        "# When the parent function is ready to add a word to the output tweet, this function helps by checking to see if there are any contractions or abbreviations \n",
        "# that need to be transformed to their \"expanded\" forms first.\n",
        "# ---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "def evaluate_word(word, contraction_mapping=contraction_map, sms_mapping=sms_map):\n",
        "\n",
        "  output_word = \"\"\n",
        "\n",
        "  if word.lower() in contraction_mapping:                                       # Check if this word is a contraction that can be expanded. If so, add the expanded form to the output tweet.\n",
        "    output_word = output_word + \" \" + contraction_mapping[word.lower()]\n",
        "  elif word.lower() in sms_mapping:                                             # Check if this word is an abbreviation that can be expanded. If so, add the expanded form to the output tweet.\n",
        "    output_word = output_word + \" \" + sms_mapping[word.lower()]          \n",
        "  elif word != \"\":                                                              # If this word is not a contraction or abbreviation, and the word is not empty, add the word to the output tweet.\n",
        "    output_word = output_word + \" \" + word\n",
        "\n",
        "  if word.startswith('#') and word.count(\"#\") > 1:                              # If the word is of the form #text, check to see if it is multiple hashtags stuck together e.g. #text1#text2, if so, split them apart.\n",
        "    output_word = split_chained_hashtags(word)\n",
        "\n",
        "  if word.count('\\'') >= 1:                                                     # Make sure there are not any apostrophes left in the word(s), if there are, remove them.\n",
        "    output_word = output_word.replace('\\'', \"\")\n",
        "\n",
        "  return output_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEJuUbqossWP"
      },
      "source": [
        "# ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# This function is used to preprocess tweets to remove contractions, punctuation and abbreviations before spaCy performs the tokenization and lemmatization process.\n",
        "# ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def fix_contractions_punctuation_abbreviation(input_tweet, contraction_mapping=contraction_map, sms_mapping=sms_map):\n",
        "\n",
        "  punctuation = \".&\\()*+,-./:;<=>\\\"!?[\\\\]^_`{|}~\"\n",
        "  word = \"\" \n",
        "  output_tweet = \"\"\n",
        "\n",
        "  for char in input_tweet:                                                      # Iterate through every character in the tweet.\n",
        "\n",
        "    if char != \" \":                                                             # If we haven't hit a white space, continue on to keep building up a word.\n",
        "      if char in punctuation:                                                   # If we hit a punctuation mark, the current word is over.                                      \n",
        "        word = evaluate_word(word)                                              # Check if this word needs to be transformed (i.e. handle contractions, abbreviations, etc.)\n",
        "        output_tweet = output_tweet + \" \" + word                                # Add the appropriate word to the output tweet.\n",
        "        word = \"\"                                              \n",
        "      else:                                                                     # If this character was not a space or a punctuation mark, continue building up the current word.\n",
        "        word = word + char\n",
        "    \n",
        "    else:                                                                       # If we reached a white space, the current word is over. \n",
        "      word = evaluate_word(word)                                                # Evaluate the word for contractions, abbreviations etc. Add the appropriate word to the output tweet.\n",
        "      output_tweet = output_tweet + \" \" + word\n",
        "      word = \"\"\n",
        "\n",
        "  # If we are about to exit but still have one last word to add.\n",
        "  if word != \"\":                                                                # If the final character is not a whitespace, we may be ready to exit the function but still have one \n",
        "    word = evaluate_word(word)                                                  # remaining word to add to the output tweet. This section handles that scenario.\n",
        "    output_tweet = output_tweet + \" \" + word\n",
        "    word = \"\"\n",
        "\n",
        "  output_tweet = output_tweet.lstrip().rstrip()                                 # Strip any excess white space on the left and right edges of the word.\n",
        "  output_tweet = re.sub(\" +\", \" \", output_tweet)                                # Replace any duplicate white spaces in the middle (possible chain of words from expanding) with a single white space.\n",
        "\n",
        "  return output_tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59zj3PyEtgOK"
      },
      "source": [
        "# --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# Inputs: An unprocessed tweet string\n",
        "# Outputs: A list of clean tweet tokens\n",
        "#\n",
        "# This function performs cleaning, tokenization and lemmatization on each tweet one at a time. This function can be used to clean the entire dataset of tweets\n",
        "# all at once by calling it with a pandas .apply function. The process for cleaning a single tweet is as follows:\n",
        "#\n",
        "# Step 1: Preprocessing\n",
        "#         - Ensure common contractions are replaced by its expanded word form.\n",
        "#         - Ensure common \"SMS speak\" slang is replaced by its expanded word form.\n",
        "#         - Ensure all punctuation marks are removed (retain hashtag symbols connected to text).\n",
        "#         - Ensure any instances of multiple hashtags stuck together are split apart with white space.\n",
        "#         \n",
        "# Step 2: Tokenization with spaCy. \n",
        "#         - Convert the clean tweet string to a spaCy doc object to allow access to tokens and lemmas.\n",
        "#         - Use spaCy to take the \"lemma\" of each individual token. \n",
        "#         - Reduce the number of unqiue words by changing all \"lemmas\" to lowercase.\n",
        "#         - Filter out any '@User' tokens, as they are too generic to provide any sentiment insight.\n",
        "#         - Filter out any lemmatized tokens that are in the list of \"stop words\" (words that are too generic to provide sentiment insight).\n",
        "#         - Add the lemmatized token to the output list.\n",
        "#\n",
        "# Note: Many of the above steps are designed to minimize the number of unique tokens without losing valuable sentiment information.\n",
        "#       Minimizing the number of unique tokens is desirable because this will lead to a matrix with lower dimensionality during the\n",
        "#       modeling phases (i.e. less unique words helps minimze impacts from the curse of dimensionality, as significantly more data is\n",
        "#       is needed to generalize well in higher dimensional spaces).\n",
        "#\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def tweet_cleaner(dirty_tweet, nlp):\n",
        "\n",
        "  # Preprocess before tokenization.\n",
        "  partially_clean_tweet = fix_contractions_punctuation_abbreviation(dirty_tweet)\n",
        "\n",
        "  # Get the list of default stop words in the pretrained spaCy model.\n",
        "  stop_words = nlp.Defaults.stop_words\n",
        "\n",
        "  # Turn the cleaned tweet into a spaCy doc object so we can access the tokens and lemmas. \n",
        "  tweet_doc = nlp(partially_clean_tweet)\n",
        "\n",
        "  clean_tweet_tokens = []\n",
        "  for token in tweet_doc:\n",
        "    if token.lemma_ == \"-PRON-\":                                # -PRON- indicates that spaCys lemmatizer flagged this work as a generic pronoun.\n",
        "      token_txt = token.lemma_.lower().strip('-')               # If the lemma of the token is -PRON- strip off the \"-\"'s and make the word lowercase.\n",
        "      clean_tweet_tokens.append(token_txt)                      # Add the clean, lemmatized token to the output list.\n",
        "    \n",
        "    # \n",
        "    elif token.lemma_ != '@user' and token.lemma_ not in stop_words and token.lemma_ != \"\" and token.lemma_ != \" \":\n",
        "      token_txt = token.lemma_.lower().strip()               \n",
        "      clean_tweet_tokens.append(token_txt)\n",
        "  \n",
        "  return clean_tweet_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPD6wQ9IzfE7"
      },
      "source": [
        "#-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# When performed using a pandas .apply function, the \"tweet_cleaner\" function shown above will create a new DataFrame column that contains of a list of clean, lemmatized tokens. \n",
        "#\n",
        "# Having the clean tweet tokens stored in a python list is desireable for situations where the list will be accessed within the same notebook, however if the intent is to save the DataFrame as a .csv\n",
        "# file for use in a later notebook this is not a good plan because when a DataFrame is saved to .csv all of its contents get converted to strings. This means a DataFrame column that contains a list\n",
        "# will become a string formatted as '[item1, item2, item3]'. This is clearly undesirable as the important information becomes unecessarily cluttered with commas and brackets.\n",
        "#\n",
        "# This function can be used to convert the DataFrame column containing a list of clean lemmatized tokens (created by tweet cleaner) into a single string. This string can then easily be retokenized\n",
        "# in a following notebook by splitting at white spaces.\n",
        "# \n",
        "# Additionally, this function looks for and handles rare circumstances where a clean lemmatized token consists only of a hashtag with no attached text. Without the text portion these tokens\n",
        "# do not have any valuable sentiment information, and are therefore removed.\n",
        "#\n",
        "#-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "def create_clean_tweet_string(clean_tokens):\n",
        "  clean_tweet_string = \"\"\n",
        "  for token in clean_tokens:\n",
        "\n",
        "    # Some hashtags with no words have not been cleaned yet, filter them here, \n",
        "    # as a hashtag with no words attached is not sentiment laden.\n",
        "    if token != \"#\":\n",
        "      clean_tweet_string = clean_tweet_string + token + \" \"\n",
        "  \n",
        "  clean_tweet_string = clean_tweet_string.strip()\n",
        "\n",
        "  return clean_tweet_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJu0gTwr4_aO"
      },
      "source": [
        "# Create a new DataFrame column that contains a list of clean, lemmatized tokens.\n",
        "tweet_df['Fully_Clean_Tweet_Tokenized'] = tweet_df['tweet_emoji_cleaned'].apply(tweet_cleaner, args=(nlp,))\n",
        "\n",
        "# Use the column containing a list of clean, lemmatized tokens to create a column containing the cleaned tweet as a single string.\n",
        "tweet_df['Clean_Tweet'] = tweet_df['Fully_Clean_Tweet_Tokenized'].apply(create_clean_tweet_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "id": "8ccKx-TI5B_X",
        "outputId": "fc194e83-8c55-4e4d-a8cf-e48c51194637"
      },
      "source": [
        "#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# After all of the above data cleaning steps, there are several tweets where no text is left at all. (Everything in the tweet was deemed too generic to provide valuable training information\n",
        "# for hate speech identification, e.g. stop words). \n",
        "#\n",
        "# Note: Several of these do contain emojis, so if the emoji sentiment dictionary were ever expanded there is a chance that some of these tweets may not be empty after data cleaning.\n",
        "#\n",
        "# For now, since an empty string tweet is not useful training or testing data, these rows are being dropped. Additionally, this dataset contains a significantly higher number of \n",
        "# non hate speech examples as compared to hate speech examples. All of these rows being dropped are non-hate speech examples, which is even further justification that removing them\n",
        "# will not have a negative impact on model performance.\n",
        "#\n",
        "#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Displaying rows where the tweet string is empty after the cleaning steps.\n",
        "tweet_df.loc[tweet_df['Clean_Tweet'] == \"\", :]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_emoji_cleaned</th>\n",
              "      <th>Fully_Clean_Tweet_Tokenized</th>\n",
              "      <th>Clean_Tweet</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2261</th>\n",
              "      <td>0</td>\n",
              "      <td>always be   ð½ð¢ð®ð¢ð¢ð§</td>\n",
              "      <td>always be</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2283</th>\n",
              "      <td>0</td>\n",
              "      <td>well done @user</td>\n",
              "      <td>well done @user</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2742</th>\n",
              "      <td>0</td>\n",
              "      <td>to show -  -</td>\n",
              "      <td>to show -  -</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4800</th>\n",
              "      <td>0</td>\n",
              "      <td>@user @user @user @user @user @user @user @us...</td>\n",
              "      <td>@user @user @user @user @user @user @user @us...</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5096</th>\n",
              "      <td>0</td>\n",
              "      <td>i am really  â¢</td>\n",
              "      <td>i am really</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5236</th>\n",
              "      <td>0</td>\n",
              "      <td>@user @user @user @user @user @user @user @us...</td>\n",
              "      <td>@user @user @user @user @user @user @user @us...</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5585</th>\n",
              "      <td>0</td>\n",
              "      <td>@user always be   ð¡ð¶ð±ð´ð±ð¡ð...</td>\n",
              "      <td>@user always be</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6128</th>\n",
              "      <td>0</td>\n",
              "      <td>no @user hasnt</td>\n",
              "      <td>no @user hasnt</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6646</th>\n",
              "      <td>0</td>\n",
              "      <td>@user  well said</td>\n",
              "      <td>@user  well said</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7820</th>\n",
              "      <td>0</td>\n",
              "      <td>@user always be   ð½ð¢ð®ð¢ð¢ð§</td>\n",
              "      <td>@user always be</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8163</th>\n",
              "      <td>0</td>\n",
              "      <td>@user still  !</td>\n",
              "      <td>@user still  !</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8771</th>\n",
              "      <td>0</td>\n",
              "      <td>@user @user @user still</td>\n",
              "      <td>@user @user @user still</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9035</th>\n",
              "      <td>0</td>\n",
              "      <td>@user very   ...</td>\n",
              "      <td>@user very   ...</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9131</th>\n",
              "      <td>0</td>\n",
              "      <td>got this on</td>\n",
              "      <td>got this on</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12509</th>\n",
              "      <td>0</td>\n",
              "      <td>@user is @user gone?!</td>\n",
              "      <td>@user is @user gone?!</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12591</th>\n",
              "      <td>0</td>\n",
              "      <td>why is this us...   @user</td>\n",
              "      <td>why is this us...   @user</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13316</th>\n",
              "      <td>0</td>\n",
              "      <td>@user well said</td>\n",
              "      <td>@user well said</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14223</th>\n",
              "      <td>0</td>\n",
              "      <td>enough is enough.</td>\n",
              "      <td>enough is enough.</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15435</th>\n",
              "      <td>0</td>\n",
              "      <td>ðð» that is all....</td>\n",
              "      <td>that is all....</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20262</th>\n",
              "      <td>0</td>\n",
              "      <td>very very very</td>\n",
              "      <td>very very very</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21649</th>\n",
              "      <td>0</td>\n",
              "      <td>@user @user @user   much!</td>\n",
              "      <td>@user @user @user   much!</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28514</th>\n",
              "      <td>0</td>\n",
              "      <td>@user @user @user   to be here</td>\n",
              "      <td>@user @user @user   to be here</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       label  ... Clean_Tweet\n",
              "id            ...            \n",
              "2261       0  ...            \n",
              "2283       0  ...            \n",
              "2742       0  ...            \n",
              "4800       0  ...            \n",
              "5096       0  ...            \n",
              "5236       0  ...            \n",
              "5585       0  ...            \n",
              "6128       0  ...            \n",
              "6646       0  ...            \n",
              "7820       0  ...            \n",
              "8163       0  ...            \n",
              "8771       0  ...            \n",
              "9035       0  ...            \n",
              "9131       0  ...            \n",
              "12509      0  ...            \n",
              "12591      0  ...            \n",
              "13316      0  ...            \n",
              "14223      0  ...            \n",
              "15435      0  ...            \n",
              "20262      0  ...            \n",
              "21649      0  ...            \n",
              "28514      0  ...            \n",
              "\n",
              "[22 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndj3rXe56T8r"
      },
      "source": [
        "# Dropping the rows where the clean tweet is an empty string.\n",
        "index_values_to_drop = tweet_df[tweet_df['Clean_Tweet'] == \"\"].index\n",
        "tweet_df.drop(index_values_to_drop, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "3ZKR_HYB6WPj",
        "outputId": "c2239ef7-4178-462c-bb1b-06557e3b7acf"
      },
      "source": [
        "# Verify the DataFrame no longer has any rows where the clean tweet is an empty string.\n",
        "tweet_df.loc[tweet_df['Clean_Tweet'] == \"\", :]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_emoji_cleaned</th>\n",
              "      <th>Fully_Clean_Tweet_Tokenized</th>\n",
              "      <th>Clean_Tweet</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [label, tweet, tweet_emoji_cleaned, Fully_Clean_Tweet_Tokenized, Clean_Tweet]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ9h0itLAU0G"
      },
      "source": [
        "# Fixing index after dropping rows, so the index does not skip numbers.\n",
        "tweet_df = tweet_df.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "h79yAs1JtlqX",
        "outputId": "e2f56420-bbb1-48e6-8cc2-a34cb8e2f03c"
      },
      "source": [
        "# Save the dataframe with the clean tweet string to a .csv file for use in later notebooks.\n",
        "tweet_df.to_csv(path_or_buf=\"/content/drive/MyDrive/Programming/Colab Notebooks/Coding_Dojo/Twitter_Sentiment_Project/intermediate_output_files/train_tweets_spacy_clean.csv\", index=False)\n",
        "\n",
        "tweet_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_emoji_cleaned</th>\n",
              "      <th>Fully_Clean_Tweet_Tokenized</th>\n",
              "      <th>Clean_Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@user when a father is dysfunctional and is s...</td>\n",
              "      <td>@user when a father is dysfunctional and is s...</td>\n",
              "      <td>[father, dysfunctional, significant, selfish, ...</td>\n",
              "      <td>father dysfunctional significant selfish pron ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
              "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
              "      <td>[thank, #lyft, credit, use, cause, pron, offer...</td>\n",
              "      <td>thank #lyft credit use cause pron offer wheelc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>bihday your majesty</td>\n",
              "      <td>bihday your majesty</td>\n",
              "      <td>[bihday, pron, majesty]</td>\n",
              "      <td>bihday pron majesty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>#model   i love u take with u all the time in ...</td>\n",
              "      <td>#model   i love u take with u all the time in ...</td>\n",
              "      <td>[#model, love, pron, pron, time, pron, happy, ...</td>\n",
              "      <td>#model love pron pron time pron happy love hap...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "      <td>[factsguide, society, #motivation]</td>\n",
              "      <td>factsguide society #motivation</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  ...                                        Clean_Tweet\n",
              "0      0  ...  father dysfunctional significant selfish pron ...\n",
              "1      0  ...  thank #lyft credit use cause pron offer wheelc...\n",
              "2      0  ...                                bihday pron majesty\n",
              "3      0  ...  #model love pron pron time pron happy love hap...\n",
              "4      0  ...                     factsguide society #motivation\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    }
  ]
}
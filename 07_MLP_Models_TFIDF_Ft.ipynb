{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_MLP_Models_TFIDF_Ft.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMNmeRt72NtFoaamFge9XUK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BradenAnderson/Twitter-Sentiment-Analysis/blob/main/07_MLP_Models_TFIDF_Ft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7HbkjmpdSbt"
      },
      "source": [
        "### This notebook contains the code to perform hyperparameter tuning on Multilayer Perceptron Models that utilize custom TFIDF weighted fastText word vectors as their inputs.\n",
        "\n",
        "### Displaying and reviewing the search results is done in the 07_fastText_TFIDF_Modeling_Analysis notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVUfaOVuqGDg",
        "outputId": "8055f89d-aa71-4195-94f1-2366ac843657"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPgkEtDtq0LF"
      },
      "source": [
        "! git clone https://github.com/facebookresearch/fastText.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1n-vk_wvq0kp"
      },
      "source": [
        "! pip install /content/fastText"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwLhYN3yqPCQ"
      },
      "source": [
        "import dill as pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import fasttext\n",
        "from fasttext.FastText import load_model\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, train_test_split, cross_validate, cross_val_predict\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer, CountVectorizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, SCORERS, multilabel_confusion_matrix, make_scorer, roc_curve, roc_auc_score, f1_score\n",
        "\n",
        "pd.set_option('display.max_rows', 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "dV9MnCy2rGZ6",
        "outputId": "03e03ce5-c5e2-401b-af77-78f62b4dc282"
      },
      "source": [
        "filepath= \"/content/drive/MyDrive/Programming/Colab Notebooks/Coding_Dojo/Twitter_Sentiment_Project/intermediate_output_files/vader_full_preprocessing_model_droppedlt3.csv\"\n",
        "\n",
        "tweet_df = pd.read_csv(filepath)\n",
        "\n",
        "tweet_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "      <th>Clean_Tweet</th>\n",
              "      <th>Sentence_Level_pos_Score</th>\n",
              "      <th>Sentence_Level_neg_Score</th>\n",
              "      <th>Sentence_Level_neu_Score</th>\n",
              "      <th>Sentence_Level_compound_Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@user when a father is dysfunctional and is s...</td>\n",
              "      <td>father dysfunctional significant selfish pron ...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.789</td>\n",
              "      <td>0.58520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
              "      <td>thank #lyft credit use cause pron offer wheelc...</td>\n",
              "      <td>0.157</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.843</td>\n",
              "      <td>1.33525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>bihday your majesty</td>\n",
              "      <td>bihday pron majesty</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>#model   i love u take with u all the time in ...</td>\n",
              "      <td>#model love pron pron time pron happy love hap...</td>\n",
              "      <td>0.194</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.806</td>\n",
              "      <td>1.36245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "      <td>factsguide society #motivation</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  ... Sentence_Level_compound_Score\n",
              "0      0  ...                       0.58520\n",
              "1      0  ...                       1.33525\n",
              "2      0  ...                       1.00000\n",
              "3      0  ...                       1.36245\n",
              "4      0  ...                       1.00000\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w2GDkX9J5ZW"
      },
      "source": [
        "tweet_df['Clean_Word_Lists'] = tweet_df['Clean_Tweet'].apply(lambda tweet : tweet.split(' '))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-54QrDoYrO7W"
      },
      "source": [
        "# Set up fastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI5an199rW5N"
      },
      "source": [
        "# Read in a csv file that contains a string with every unqiue word found in the set of tweets.\n",
        "unique_df = pd.read_csv(\"/content/drive/MyDrive/Programming/Colab Notebooks/Coding_Dojo/Twitter_Sentiment_Project/intermediate_output_files/unique_words.csv\")\n",
        "\n",
        "# Grab the long string of unqiue words from the dataframe.\n",
        "unique_words = unique_df.loc[unique_df.index == 0, 'Unique_Words'].to_numpy()[0]\n",
        "\n",
        "# Split the string at white spaces to get a list of unique words.\n",
        "unique_words = unique_words.split(\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nak2lMQ0raZT"
      },
      "source": [
        "# Path to where the fastText word vector model is saved.\n",
        "word_vector_model_filepath = r\"/content/drive/MyDrive/Programming/Colab Notebooks/Coding_Dojo/Twitter_Sentiment_Project/fastText_Models/wv_model_dlt3.bin\"\n",
        "\n",
        "# Load the model of word vector representations that was trained in the previous section.\n",
        "ft_model = load_model(word_vector_model_filepath)\n",
        "\n",
        "# Create a dictionary mapping each unique word to its fastText word vector.\n",
        "word_vector_dictionary = {word : ft_model.get_word_vector(word) for word in unique_words}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_GoTNnPrN3A"
      },
      "source": [
        "class TfidfEmbeddingVectorizer(object):\n",
        "\n",
        "    def __init__(self, ft_wv):\n",
        "\n",
        "        self.ft_wv = ft_wv\n",
        "        self.word2weight = None\n",
        "\n",
        "        if len(ft_wv) > 0:\n",
        "            self.dim = ft_wv[next(iter(unique_words))].shape[0]\n",
        "        else:\n",
        "            self.dim=0\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "\n",
        "        tfidf = TfidfVectorizer(analyzer=lambda word : word)\n",
        "        tfidf.fit(X)\n",
        "\n",
        "        #-------------------------------------------------------------------------------------------------------------------------------------\n",
        "        # The .idf_ attribute is a vector that contains the inverse document frequency values for each word in the vocabuary.\n",
        "        #\n",
        "        # The .vocabulary_ attribute is a list of tuples of the form (word, index) where index is the location in the .idf_ list \n",
        "        # where the inverse document frequency value for that word is stored.\n",
        "        #\n",
        "        # As the number of documents a particular word shows up in increases, its idf(t) (.idf_) value will decrease. \n",
        "        # idf(t) = log( (1+n) / (1 + df(t)) ) + 1, where n = number of documents (tweets) and df(t) = number of documents that contain word t. \n",
        "        # --------------------------------------------------------------------------------------------------------------------------------------\n",
        "        \n",
        "        # Save the maximum value in the list of inverse document frequencies. This corresponds to the word that shows up in the\n",
        "        # least number of documents (tweets). This can be used as a default value to return if we ever try to find the inverse document frequency\n",
        "        # for a word that was not in the training set (the assumption being made is that if a word was not in the training set, that it\n",
        "        # is at least as uncommon as the most uncommon word in the training set). \n",
        "        max_idf = max(tfidf.idf_)\n",
        "\n",
        "        # 1) Use the .idf_ list and the .vocabulary_ dictionary to create a new dictionary that maps each word to its idf value. \n",
        "        # 2) Since this is being created as a defaultdict, if we ever try to get the idf value using a word (key) that was not in the\n",
        "        #    training set, this dictionary will return the default value (idf of the most uncommon word) rather than throw a key error.\n",
        "        self.word2weight = defaultdict(lambda: max_idf,  [(word, tfidf.idf_[idf_index]) for word, idf_index in tfidf.vocabulary_.items()])\n",
        "    \n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "\n",
        "      # List to hold the average tfidf fast text vector for each tweet.\n",
        "      mean_tfidf_vector_for_each_tweet = []\n",
        "\n",
        "      # for every tweet in the training data.\n",
        "      for clean_tweet_token_list in X: \n",
        "\n",
        "        # create a list to hold the tfidf fastText vectors for each word in this tweet.\n",
        "        this_tweet_tfidf_vectors = []\n",
        "          \n",
        "        # For every word in the tweet.\n",
        "        for word in clean_tweet_token_list: \n",
        "            \n",
        "          # Initialize the fastText vector for this word to be the zero vector.\n",
        "          fastText_vector = np.zeros(self.dim)\n",
        "\n",
        "          # Check if we have a fastText word vector for this word, if we do, update fasText_vector to be the correct value.\n",
        "          if word in self.ft_wv:\n",
        "            fastText_vector = self.ft_wv[word]\n",
        "\n",
        "          # Calculate the tfidf_vector as the words fastText vector multiplied by its idf weight.\n",
        "          tfidf_vector = fastText_vector * self.word2weight[word] \n",
        "\n",
        "          # Add the tfidf_vector for this word to the list of tfidf_vectors for this tweet.\n",
        "          this_tweet_tfidf_vectors.append(tfidf_vector)\n",
        "\n",
        "        # To get a single vector that represents the entire tweet, take the mean of the tfidf vectors for all words in the tweet.\n",
        "        mean_tfidf_vector = np.mean(this_tweet_tfidf_vectors, axis=0)\n",
        "        \n",
        "        # Add the mean tfidf vector for this tweet to the list of mean tfidf vectors for all tweets.\n",
        "        mean_tfidf_vector_for_each_tweet.append(mean_tfidf_vector)\n",
        "\n",
        "      return np.array(mean_tfidf_vector_for_each_tweet)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC0Q8INbrc6T"
      },
      "source": [
        "# Perform GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KusMktvcrvPo"
      },
      "source": [
        "'''\n",
        "X = tweet_df.loc[:, ['Clean_Word_Lists']].to_numpy().ravel()\n",
        "y = tweet_df.loc[:, 'label'].to_numpy().ravel()\n",
        "\n",
        "multi_layer_perceptron = MLPClassifier()\n",
        "\n",
        "model_pipeline = Pipeline([(\"ft_word_vectorizer\", TfidfEmbeddingVectorizer(word_vector_dictionary)),\n",
        "                           ('MLP', multi_layer_perceptron)])\n",
        "\n",
        "parameter_grid = [{'MLP__hidden_layer_sizes' : [100, 200, 300],\n",
        "                   'MLP__activation' : ['relu', 'logistic'], \n",
        "                   'MLP__alpha' : [0.0001, 0.0005]}, \n",
        "                  {'MLP__hidden_layer_sizes' : [100, 200, 300],\n",
        "                   'MLP__activation' : ['relu', 'logistic'],\n",
        "                   'MLP__solver' : ['sgd'], \n",
        "                   'MLP__learning_rate' : ['adaptive'], \n",
        "                   'MLP__alpha' : [0.0001, 0.0005]}]\n",
        "\n",
        "score_types = {'f1_score' : make_scorer(f1_score), 'sensitivity' : make_scorer(recall_score), 'specificity' : make_scorer(recall_score, pos_label=0),\n",
        "               'AUC_ROC' : 'roc_auc', 'ROC_AUC_Score' : make_scorer(roc_auc_score), 'accuracy' : 'accuracy', 'precision' : make_scorer(precision_score)}\n",
        "\n",
        "gs = GridSearchCV(estimator=model_pipeline, param_grid=parameter_grid, scoring=score_types, refit='f1_score', n_jobs=-1)\n",
        "\n",
        "gs.fit(X,y)\n",
        "\n",
        "PATH = \"/content/drive/MyDrive/Programming/Colab Notebooks/Coding_Dojo/Twitter_Sentiment_Project/pickle_gridsearch_ft/gs_mlp_tfidf_ft.pkl\"\n",
        "\n",
        "with open(PATH, 'wb') as file:\n",
        "  pickle.dump(gs, file)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z4hhNyvn4yZ"
      },
      "source": [
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import RandomOverSampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b0HKwi2n0m2"
      },
      "source": [
        "'''\n",
        "X = tweet_df.loc[:, ['Clean_Word_Lists']].to_numpy().ravel()\n",
        "y = tweet_df.loc[:, 'label'].to_numpy().ravel()\n",
        "\n",
        "multi_layer_perceptron = MLPClassifier()\n",
        "\n",
        "random_os = RandomOverSampler()\n",
        "\n",
        "model_pipeline = Pipeline([(\"ft_word_vectorizer\", TfidfEmbeddingVectorizer(word_vector_dictionary)),\n",
        "                           ('overSampler', random_os),\n",
        "                           ('MLP', multi_layer_perceptron)])\n",
        "\n",
        "parameter_grid = [{'MLP__hidden_layer_sizes' : [300, 350, 400],\n",
        "                   'MLP__activation' : ['logistic'], \n",
        "                   'MLP__alpha' : [0.0004, 0.0005, 0.0006],\n",
        "                   'overSampler__sampling_strategy' : ['auto', 0.6, 0.4]}]\n",
        "\n",
        "score_types = {'f1_score' : make_scorer(f1_score), 'sensitivity' : make_scorer(recall_score), 'specificity' : make_scorer(recall_score, pos_label=0),\n",
        "               'AUC_ROC' : 'roc_auc', 'ROC_AUC_Score' : make_scorer(roc_auc_score), 'accuracy' : 'accuracy', 'precision' : make_scorer(precision_score)}\n",
        "\n",
        "gs = GridSearchCV(estimator=model_pipeline, param_grid=parameter_grid, scoring=score_types, refit='f1_score', n_jobs=-1)\n",
        "\n",
        "gs.fit(X,y)\n",
        "\n",
        "PATH = \"/content/drive/MyDrive/Programming/Colab Notebooks/Coding_Dojo/Twitter_Sentiment_Project/pickle_gridsearch_ft/gs_mlp_ros_tfidf_ft.pkl\"\n",
        "\n",
        "with open(PATH, 'wb') as file:\n",
        "  pickle.dump(gs, file)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}